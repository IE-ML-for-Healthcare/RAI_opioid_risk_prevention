{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b54679",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook is part of a machine learning for healthcare exercise, focusing on using the Responsible AI (RAI) package to enhance clinical decision-making. The toolkit will be used to analyze opioid use disorder (OD) risk, with three key objectives:\n",
    "\n",
    "1. Analyze Errors and Explore Interpretability of Models: We will run Interpret-Community’s global explainers to generate feature importance insights and visualize model errors with the Error Analysis dashboard\n",
    "\n",
    "2. Plan real-world action through counterfactual and causal analysis: By leveraging counterfactual examples and causal inference, we will explore decision-making strategies based on opioid prescription patterns and patient comorbidities to understand possible interventions and their impacts\n",
    "\n",
    "3. Assess addiction risk predictions: A classification model trained on patient-level features (income, surgeries, opioid prescription days, and comorbidities A–V) will be evaluated to examine its performance in predicting risk of opioid use disorder and to inform prevention strategies\n",
    "\n",
    "**The goal is to provide non-trivial insights for clinical decision making, leveraging machine learning paired with responsible AI tools, to improve patient outcomes in the healthcare context.**\n",
    "\n",
    "Based on notebooks from the [Responsible AI toolkit](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934c151",
   "metadata": {},
   "source": [
    "# 2. Data Set Characteristics\n",
    "\n",
    "Number of Instances: patient-level records (rows)\n",
    "\n",
    "Number of Attributes: 20 predictive attributes and 1 target class\n",
    "\n",
    "Attribute Information:\n",
    "- OD (target): whether the patient had an opioid use disorder diagnosis (binary: 1 = yes, 0 = no)\n",
    "- Low_inc: low income flag (1 = low income, 0 = not low income)\n",
    "- Surgery: whether the patient underwent major surgery in the 2 years\n",
    "- rx_ds: number of days of prescribed opioids in the 2 years\n",
    "- A: infectious diseases group A (binary flag)\n",
    "- B: infectious diseases group B\n",
    "- C: malignant neoplasm\n",
    "- D: benign neoplasm\n",
    "- E: endocrine conditions\n",
    "- F: mental and behavioral health conditions (excluding opioid-related)\n",
    "- H: ear conditions\n",
    "- I: circulatory system conditions\n",
    "- J: respiratory system conditions\n",
    "- K: digestive system conditions\n",
    "- L: skin conditions\n",
    "- M: musculoskeletal system conditions\n",
    "- N: urinary system conditions\n",
    "- R: other signs and symptoms\n",
    "- S: injuries\n",
    "- T: burns and toxic conditions\n",
    "- V: external trauma conditions\n",
    "\n",
    "class:\n",
    "- OD = 1: patient identified with opioid use disorder in the 2 years\n",
    "- OD = 0: patient without opioid use disorder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da36d3",
   "metadata": {},
   "source": [
    "# 3. Setup\n",
    "- responsibleai and raiwidgets provide RAIInsights and the dashboard\n",
    "- fairlearn provides fairness metrics and mitigation algorithms used under the hood\n",
    "- imbalanced-learn offers resampling utilities if you want to experiment with imbalance mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,           # Measures the ability of the model to distinguish between classes (higher is better)\n",
    "    average_precision_score, # Computes the average precision for precision-recall curves (useful for imbalanced data)\n",
    "    brier_score_loss,        # Measures the mean squared difference between predicted probabilities and actual outcomes (lower is better)\n",
    "    log_loss,                # Penalizes false classifications with a focus on probability estimates (lower is better)\n",
    "    confusion_matrix,        # Summarizes true/false positives/negatives for classification predictions\n",
    "    precision_score,         # Proportion of positive identifications that were actually correct (TP / (TP + FP))\n",
    "    recall_score,            # Proportion of actual positives that were correctly identified (TP / (TP + FN))\n",
    "    RocCurveDisplay,         \n",
    "    PrecisionRecallDisplay,  \n",
    ")\n",
    "\n",
    "# Fairness utilities\n",
    "try:\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate,\n",
    "        true_positive_rate,\n",
    "        false_positive_rate,\n",
    "    )\n",
    "    _FAIRLEARN = True\n",
    "except Exception:\n",
    "    _FAIRLEARN = False\n",
    "\n",
    "# Optional Responsible AI dashboard\n",
    "try:\n",
    "    from responsibleai import RAIInsights\n",
    "    from raiwidgets import ResponsibleAIDashboard\n",
    "    _RAI = True\n",
    "except Exception:\n",
    "    _RAI = False\n",
    "\n",
    "# Course utilities for transparency and thresholding\n",
    "from utils import (\n",
    "    positive_scores,\n",
    "    auc_report,\n",
    "    tradeoff_table,\n",
    "    pick_threshold_cost,\n",
    "    pick_threshold_recall_floor,\n",
    "    pick_threshold_workload,\n",
    "    summary_at_threshold,\n",
    "    plot_recall_floor_curves,\n",
    "    plot_cumulative_recall_at_threshold,\n",
    "    plot_topk_at_threshold,\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc05cf",
   "metadata": {},
   "source": [
    "# 4. Data Load & preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f881f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV (update if needed for your system)\n",
    "DATA_PATH = \"./Data/opiod_raw_data.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome prevalence and missingness\n",
    "\n",
    "TARGET = \"OD\"  # target: 1 = opioid use disorder diagnosis, 0 = none\n",
    "\n",
    "# Outcome prevalence\n",
    "counts = df_raw[TARGET].value_counts(dropna=False)\n",
    "prevalence_percent = counts[1] / counts.sum() * 100\n",
    "positives_per_1000 = counts[1] / counts.sum() * 1000\n",
    "\n",
    "print(\"Outcome counts:\")\n",
    "print(counts)\n",
    "print(f\"\\nPrevalence: {prevalence_percent:.2f}%\")\n",
    "print(f\"Patients with OD per 1000: {positives_per_1000:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142570c9",
   "metadata": {},
   "source": [
    "## 4.1. Basic cleaning and schema alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9316e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the raw DataFrame\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [c.strip().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "              for c in df.columns]\n",
    "\n",
    "# Drop ID column\n",
    "if df.shape[1] > 0:\n",
    "    df = df.drop(columns=[df.columns[0]])\n",
    "\n",
    "# Harmonize known aliases\n",
    "if \"SURG\" in df.columns and \"Surgery\" not in df.columns:\n",
    "    df = df.rename(columns={\"SURG\": \"Surgery\"})\n",
    "\n",
    "# Expected columns from the data dictionary\n",
    "expected_cols = [\n",
    "    \"OD\", \"Low_inc\", \"Surgery\", \"rx_ds\",\n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"R\", \"S\", \"T\", \"V\"\n",
    "]\n",
    "\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# Helper to coerce various binary encodings to 0/1\n",
    "\n",
    "\n",
    "def to_binary(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype == \"O\":\n",
    "        mapped = s.astype(str).str.strip().str.lower().map({\n",
    "            \"1\": 1, \"0\": 0,\n",
    "            \"y\": 1, \"n\": 0,\n",
    "            \"yes\": 1, \"no\": 0,\n",
    "            \"true\": 1, \"false\": 0\n",
    "        })\n",
    "        s = pd.to_numeric(mapped, errors=\"coerce\")\n",
    "    else:\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return (s.fillna(0) > 0).astype(int)\n",
    "\n",
    "\n",
    "# Target is binary 0/1\n",
    "df[\"OD\"] = to_binary(df[\"OD\"])\n",
    "\n",
    "# rx_ds is numeric count of opioid prescription days\n",
    "df[\"rx_ds\"] = pd.to_numeric(df[\"rx_ds\"], errors=\"coerce\")\n",
    "\n",
    "# Binary predictors: Low_inc, Surgery, and A..V\n",
    "binary_cols = [\"Low_inc\", \"Surgery\", \"A\", \"B\", \"C\", \"D\", \"E\",\n",
    "               \"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"R\", \"S\", \"T\", \"V\"]\n",
    "df[binary_cols] = df[binary_cols].apply(to_binary)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34345072",
   "metadata": {},
   "source": [
    "# 5. Training, Validation and Testing - 80/15/5 stratified split\n",
    "\n",
    "Train–test split\n",
    "\n",
    "- Prevents “peeking” at data and overestimating performance\n",
    "- Mimics real-world deployment where models face unseen patients\n",
    "- Always evaluate on data not used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"OD\"\n",
    "num_cols = [\"rx_ds\"]\n",
    "cat_like_binary_cols = binary_cols.copy()\n",
    "\n",
    "X = df[num_cols + cat_like_binary_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Split temp into validation and test, 15% each overall\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val:\",   X_val.shape,   \"y_val:\",   y_val.shape)\n",
    "print(\"X_test:\",  X_test.shape,  \"y_test:\",  y_test.shape)\n",
    "\n",
    "# Sanity check class balance\n",
    "\n",
    "\n",
    "def _dist(s):\n",
    "    vc = s.value_counts(normalize=True).sort_index()\n",
    "    return {\"p(OD=0)\": float(vc.get(0, 0.0)), \"p(OD=1)\": float(vc.get(1, 0.0))}\n",
    "\n",
    "\n",
    "print(\"Class distribution overall:\", _dist(y))\n",
    "print(\"Class distribution train:\",   _dist(y_train))\n",
    "print(\"Class distribution val:\",     _dist(y_val))\n",
    "print(\"Class distribution test:\",    _dist(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0a7ff",
   "metadata": {},
   "source": [
    "## 5.1. Modeling pipeline, training, and calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63d7b7",
   "metadata": {},
   "source": [
    "### Setting a baseline\n",
    "A naive majority-class baseline clarifies the minimum standard any model must beat, highlighting the danger of ignoring minority patients and ensuring improvements carry meaningful weight in healthcare decision making\n",
    "\n",
    "**ROC AUC**  \n",
    "- 0.5 → no discrimination\n",
    "- 0.6–0.7 → poor\n",
    "- 0.7–0.8 → fair\n",
    "- 0.8–0.9 → good\n",
    "- ≥ 0.9 → excellent\n",
    "\n",
    "**PR AUC**  \n",
    "- Must be interpreted against event prevalence `p` in the validation set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: DummyClassifier (majority class)\n",
    "dummy_clf = DummyClassifier(\n",
    "    strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "proba_val = dummy_clf.predict_proba(X_val)\n",
    "pos_idx = int(np.where(dummy_clf.classes_ == 1)[0][0])  # index for class \"1\"\n",
    "y_score_val = np.asarray(proba_val)[:, pos_idx]\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(dummy_clf, X_val)\n",
    "metrics_dummy = auc_report(\n",
    "    y_val, y_score_val, name=\"Dummy baseline\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bc2a2",
   "metadata": {},
   "source": [
    "## 5.2. Preprocesing our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74d963",
   "metadata": {},
   "source": [
    "[Scikit-learn preprocessing](https://scikit-learn.org/stable/api/sklearn.preprocessing.html) standardizes and transforms features for modeling, including scaling, encoding, and imputation. Helping to maintain a consistent data transformation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True))\n",
    "])\n",
    "\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"bin\", binary_transformer, cat_like_binary_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",  # drops any column not in previously specified\n",
    "    verbose_feature_names_out=False  # keeps original feature names\n",
    ")\n",
    "\n",
    "# Logistic Regression baseline with variance filter\n",
    "base_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    # Eliminates features with zero variance\n",
    "    (\"varth\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"liblinear\",  # See details in course material\n",
    "        class_weight=\"balanced\",  # Adjusts weights for class imbalance\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=200\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8ccc7",
   "metadata": {},
   "source": [
    "### Basic preliminary model performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit base model\n",
    "base_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(base_clf, X_val)\n",
    "metrics_base = auc_report(y_val, y_score_val, name=\"base_clf\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4634f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five patients: raw pre-calibration score, predicted label, actual label\n",
    "\n",
    "tbl5 = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"raw_score\": positive_scores(base_clf, X_val),\n",
    "            \"actual\": y_val.loc[X_val.index].astype(int),\n",
    "        },\n",
    "        index=X_val.index,\n",
    "    )\n",
    "    .assign(predicted=lambda d: (d[\"raw_score\"] >= 0.50).astype(int))  # threshold before calibration\n",
    "    .round({\"raw_score\": 3})\n",
    "    .sample(n=10, random_state=7)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"patient\"})\n",
    "    .loc[:, [\"patient\", \"raw_score\", \"predicted\", \"actual\"]]\n",
    ")\n",
    "\n",
    "display(tbl5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the plot using variables from the preceding cells\n",
    "scores = positive_scores(base_clf, X_val)\n",
    "actuals = y_val\n",
    "\n",
    "# Separate scores for actual positive (OD=1) and negative (OD=0) cases\n",
    "scores_positive = scores[actuals == 1]\n",
    "scores_negative = scores[actuals == 0]\n",
    "\n",
    "# Generate random jitter for the x-axis\n",
    "jitter_strength = 0.03\n",
    "jitter_positive = np.random.normal(0, jitter_strength, len(scores_positive))\n",
    "jitter_negative = np.random.normal(0, jitter_strength, len(scores_negative))\n",
    "\n",
    "# 3. Create the plot\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "scatter_other = ax.scatter(jitter_negative, scores_negative, color='darkslategrey', alpha=0.3, label='Other')\n",
    "scatter_target = ax.scatter(jitter_positive, scores_positive, color='red', alpha=0.7, label='OD')\n",
    "\n",
    "# Add threshold lines\n",
    "line_50 = ax.axhline(y=0.5, color='red', linestyle='-', linewidth=2, label='Threshold = 0.5')\n",
    "line_80 = ax.axhline(y=0.7, color='orange', linestyle='--', linewidth=2, label='Threshold = 0.7')\n",
    "\n",
    "ax.set_title('Scores vs. Actuals')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_xlim(-0.2, 0.2) # Set fixed x-axis limits to control the visual spread\n",
    "ax.legend(handles=[scatter_target, scatter_other, line_50, line_80], fontsize=9, loc='upper right')\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e413ac1",
   "metadata": {},
   "source": [
    "# 6. Recalibrating the Scores\n",
    "\n",
    "**1. Reliable probabilities**  \n",
    "- Turns raw scores into real probabilities  \n",
    "- Ensures predictions match observed outcome frequencies  \n",
    "- Prevents overly high or low risk estimates  \n",
    "\n",
    "**2. Better clinical decisions**  \n",
    "- Essential when risk values guide medical choices  \n",
    "- Supports thresholds with clear clinical meaning\n",
    "- Reduces wasted clinical resources  \n",
    "\n",
    "**3. Trust and adoption**  \n",
    "- Builds trust in AI decisions  \n",
    "- Enables safer patient outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate probabilities using CV on training data\n",
    "calibrated_clf = CalibratedClassifierCV(\n",
    "    estimator=base_clf,\n",
    "    method=\"sigmoid\",\n",
    "    cv=5\n",
    ")\n",
    "calibrated_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile bands on test only\n",
    "base_clf_uncal = clone(base_clf).fit(X_train, y_train)\n",
    "p_before = base_clf_uncal.predict_proba(X_test)[:, 1]\n",
    "p_after = calibrated_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_cal = pd.DataFrame({\"Actual_OD\": y_test.values, \"Pred_before\": p_before,\n",
    "                      \"Pred_after\": p_after}, index=X_test.index)\n",
    "\n",
    "# 10 quantile bands with similar counts\n",
    "df_cal[\"Risk_band\"] = pd.qcut(df_cal[\"Pred_after\"], q=10, labels=[\n",
    "                              f\"Q{i}\" for i in range(1, 11)], duplicates=\"drop\")\n",
    "\n",
    "summary = (\n",
    "    df_cal.groupby(\"Risk_band\", observed=True)\n",
    "    .agg(Patients=(\"Actual_OD\", \"size\"),\n",
    "         Avg_pred_before=(\"Pred_before\", \"mean\"),\n",
    "         Actual_OD_rate=(\"Actual_OD\", \"mean\"),\n",
    "         Avg_pred_after=(\"Pred_after\", \"mean\"))\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "print(\"Check sizes\")\n",
    "print(\"len(X_train) =\", len(X_train), \"len(X_val) =\",\n",
    "      len(X_val), \"len(X_test) =\", len(X_test))\n",
    "print(\"Rows in table sum to\", int(summary[\"Patients\"].sum()))\n",
    "\n",
    "# Reliability plot using the same fixed risk bands summary\n",
    "print(\"Points near the diagonal mean predicted risk matches observed OD frequency\")\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\",\n",
    "         linewidth=1, label=\"Perfectly calibrated\")\n",
    "plt.scatter(summary[\"Avg_pred_before\"],\n",
    "            summary[\"Actual_OD_rate\"], label=\"Before calibration\")\n",
    "plt.scatter(summary[\"Avg_pred_after\"],\n",
    "            summary[\"Actual_OD_rate\"],  label=\"After calibration\")\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Observed OD rate\")\n",
    "plt.title(\"Calibration reliability by risk bands\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234181a1",
   "metadata": {},
   "source": [
    "### Compare base_clf vs calibrated_clf on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d490f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on discrimination equality and calibration improvements\n",
    "\n",
    "# Scores\n",
    "y_score_val_base = positive_scores(base_clf, X_val)\n",
    "y_score_val_cal = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Discrimination\n",
    "roc_base = roc_auc_score(y_val, y_score_val_base)\n",
    "pr_base = average_precision_score(y_val, y_score_val_base)\n",
    "roc_cal = roc_auc_score(y_val, y_score_val_cal)\n",
    "pr_cal = average_precision_score(y_val, y_score_val_cal)\n",
    "\n",
    "# Ranking correlation\n",
    "rho_s, _ = spearmanr(y_score_val_base, y_score_val_cal)\n",
    "tau_k, _ = kendalltau(y_score_val_base, y_score_val_cal)\n",
    "\n",
    "# Calibration\n",
    "ll_base = log_loss(y_val, np.clip(y_score_val_base, 1e-6, 1 - 1e-6))\n",
    "ll_cal = log_loss(y_val, np.clip(y_score_val_cal,  1e-6, 1 - 1e-6))\n",
    "brier_base = brier_score_loss(y_val, y_score_val_base)\n",
    "brier_cal = brier_score_loss(y_val, y_score_val_cal)\n",
    "\n",
    "# Assemble into dataframe\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"Metric\": \"ROC AUC\", \"Base\": roc_base, \"Calibrated\": roc_cal},\n",
    "    {\"Metric\": \"PR AUC\", \"Base\": pr_base, \"Calibrated\": pr_cal},\n",
    "    {\"Metric\": \"Spearman rank corr\", \"Base\": rho_s, \"Calibrated\": rho_s},\n",
    "    {\"Metric\": \"Kendall tau\", \"Base\": tau_k, \"Calibrated\": tau_k},\n",
    "    # Penalises wrong over-confident predictions\n",
    "    {\"Metric\": \"Log loss\", \"Base\": ll_base, \"Calibrated\": ll_cal},\n",
    "    {\"Metric\": \"Brier score\", \"Base\": brier_base,\n",
    "        \"Calibrated\": brier_cal},  # MSE of predicted probabilities\n",
    "])\n",
    "\n",
    "display(metrics_df.style.format({\"Base\": \"{:.3f}\", \"Calibrated\": \"{:.3f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c73af",
   "metadata": {},
   "source": [
    "# 7. Deciding where to cut off i.e. what probability is “high risk enough” to trigger an intervention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96638f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get calibrated probabilities\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Define a grid of thresholds\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "rows = []\n",
    "for thr in thresholds:\n",
    "    y_pred = (y_score_val >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    alerts_per_1000 = 1000 * np.mean(y_pred)\n",
    "    true_pos_per_1000 = 1000 * tp / len(y_val)\n",
    "    rows.append({\n",
    "        \"threshold\": thr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"alerts_per_1000\": alerts_per_1000,\n",
    "        \"true_pos_per_1000\": true_pos_per_1000,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "# Display a few candidate thresholds for discussion\n",
    "display(\n",
    "    df_thr.query(\"threshold in [0.1, 0.2, 0.3, 0.4, 0.5]\")\n",
    "         .round(3)\n",
    "         .set_index(\"threshold\")\n",
    ")\n",
    "\n",
    "# Plot workload vs threshold\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000 patients\")\n",
    "plt.title(\"Operational tradeoffs vs threshold (validation set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cd2f5",
   "metadata": {},
   "source": [
    "## 7.1. Choosing an operating threshold\n",
    "\n",
    "Models produce probabilities, but clinicians are the ones taking decisions, and carring the accountability of their actions.\n",
    "\n",
    "- Setting a threshold balances in this case, among others, between missed addiction cases and unnecessary undertreatment of pain\n",
    "- Clear rules make these tradeoffs explicit, explainable, and auditable!\n",
    "\n",
    "We will run three threshold tuning analyses:\n",
    "1. **Workload constrained threshold**  \n",
    "  Capture the most true cases without exceeding a fixed alert capacity for the clinic\n",
    "2. **Recall floor then maximize precision**  \n",
    "  Guarantee a minimum case capture for safety, then pick the threshold with the fewest false alarms\n",
    "3. **Cost based threshold (Bayes rule)**  \n",
    "  Minimize expected harm using estimated costs for false negatives and false positives\n",
    "\n",
    "Readouts to watch\n",
    "- Threshold, precision, recall, alerts per 1000 patients, true positives per 1000, false positives, false negatives\n",
    "- Connect the chosen rule to clinical policy and resource capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f08220",
   "metadata": {},
   "source": [
    "### 1. Workload constrained threshold for calibrated_clf on validation\n",
    "\n",
    "- Alerts budget: maximum alerts per 1000 patients the clinic can review without overloading resources\n",
    "- Objective: within the alerts budget, choose the threshold that yields the most true positives per 1000 so more patients at real risk are correctly flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALERTS_BUDGET = 100.0  # alerts per 1000 patients\n",
    "\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "tbl = res[\"table\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.axhline(ALERTS_BUDGET, linestyle=\"--\")\n",
    "plt.axvline(res[\"threshold\"], linestyle=\":\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000\")\n",
    "plt.title(\"Workload constrained threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "display(summary_df.round(3).set_index(\"rule\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BINS = 10  # deciles by default\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Build bands by quantiles, highest risk = band 1\n",
    "bands = pd.qcut(y_score_val, q=N_BINS, labels=False, duplicates=\"drop\")\n",
    "# qcut labels lowest=0..highest=K-1, invert so 1 is highest-risk band\n",
    "bands = (bands.max() - bands) + 1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"y_true\": y_val.astype(int),\n",
    "    \"score\": y_score_val,\n",
    "    \"band\": bands.astype(int),\n",
    "})\n",
    "\n",
    "summ = (df.groupby(\"band\", as_index=True)\n",
    "          .agg(n=(\"y_true\",\"size\"),\n",
    "               positives=(\"y_true\",\"sum\"),\n",
    "               min_score=(\"score\",\"min\"),\n",
    "               max_score=(\"score\",\"max\"))\n",
    "          .sort_index())\n",
    "\n",
    "summ[\"prevalence\"] = summ[\"positives\"] / summ[\"n\"]\n",
    "summ[\"cum_capture\"] = summ[\"positives\"].cumsum() / df[\"y_true\"].sum()\n",
    "summ[\"alerts_per_1000\"] = 1000.0 * summ[\"n\"] / len(df)\n",
    "summ[\"true_pos_per_1000\"] = 1000.0 * summ[\"positives\"] / len(df)\n",
    "\n",
    "display(summ.round(3))\n",
    "\n",
    "# Optional threshold overlay if you already chose one, else set THR=None\n",
    "THR = None  # e.g., THR = 0.23\n",
    "\n",
    "# Histogram of risk scores with decile edges\n",
    "plt.figure()\n",
    "plt.hist(df[\"score\"], bins=30)\n",
    "if THR is not None:\n",
    "    plt.axvline(THR, linestyle=\"--\")\n",
    "# draw decile boundaries\n",
    "edges = np.quantile(df[\"score\"], np.linspace(0,1,N_BINS+1))\n",
    "for x in edges:\n",
    "    plt.axvline(x, linestyle=\":\", linewidth=0.8)\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Patients\")\n",
    "plt.title(\"Risk distribution with decile boundaries\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "THR = res[\"threshold\"] # <- use the threshold from the previous step\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "ids = np.arange(len(y_val))\n",
    "\n",
    "# Sort patients by predicted risk\n",
    "order = np.argsort(-y_score_val)\n",
    "top_idx = order[:30]   # top 30 for visualization\n",
    "top_scores = y_score_val[top_idx]\n",
    "top_true = np.asarray(y_val)[top_idx].astype(int)\n",
    "\n",
    "# Split indices for TP vs FP\n",
    "tp_idx = np.where(top_true == 1)[0]\n",
    "fp_idx = np.where(top_true == 0)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(tp_idx, top_scores[tp_idx], color=\"tab:red\", label=\"True addicted (TP)\")\n",
    "plt.bar(fp_idx, top_scores[fp_idx], color=\"tab:gray\", label=\"Not addicted (FP)\")\n",
    "plt.axhline(THR, linestyle=\"--\", color=\"black\", label=f\"Threshold = {THR:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Patients ranked by predicted risk\")\n",
    "plt.ylabel(\"Predicted risk\")\n",
    "plt.title(\"Top 50 highest-risk patients on validation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195459c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "order = np.argsort(-y_score_val)\n",
    "y_sorted = np.asarray(y_val)[order].astype(int)\n",
    "\n",
    "# Cumulative recall\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "total_pos = cum_tp[-1] if cum_tp.size else 0\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "recall_curve = cum_tp / total_pos if total_pos > 0 else np.zeros_like(cum_tp)\n",
    "\n",
    "# Budget for alerts\n",
    "n_budget = int(np.ceil(ALERTS_BUDGET * len(y_val) / 1000.0))\n",
    "\n",
    "# Recall at budget\n",
    "recall_at_budget = recall_curve[n_budget - 1] if n_budget > 0 and n_budget <= len(y_val) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, recall_curve, label=\"Cumulative recall\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", color=\"red\", label=f\"Budget = {n_budget} alerts\")\n",
    "\n",
    "# Annotate recall at budget\n",
    "plt.scatter(n_budget, recall_at_budget, color=\"black\", zorder=5)\n",
    "plt.text(n_budget + 2, recall_at_budget, f\"Recall = {recall_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Cumulative capture of true cases vs alerts\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative precision at top-k alerts\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "precision_curve = cum_tp / alerts\n",
    "\n",
    "# Alerts budget scaled to validation size\n",
    "prec_at_budget = precision_curve[n_budget - 1] if 0 < n_budget <= len(y_sorted) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, precision_curve, label=\"Precision at top-k alerts\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", label=f\"Budget = {n_budget} alerts\")\n",
    "plt.scatter(n_budget, prec_at_budget, zorder=5)\n",
    "plt.text(n_budget + max(2, len(y_sorted)//100), prec_at_budget, f\"Precision = {prec_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs number of alerts\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c8fbb",
   "metadata": {},
   "source": [
    "### 2. Recall floor then maximize precision for calibrated_clf on validation\n",
    "\n",
    "- Recall floor: minimum acceptable recall set by safety policy to limit missed addiction cases\n",
    "- Precision objective: among thresholds meeting the recall floor, pick the one with highest precision to reduce unnecessary undertreatment and clinician workload\n",
    "\n",
    "Deciding which recall floor to sue:\n",
    "1. The chosen floor is a value judgment balancing patient safety vs resource burden\n",
    "2. In medicine, it's often the case that missing a true case (false negative) is often much worse than raising extra alarms (false positives)\n",
    "3. A recall floor enforces a safety guarantee: the model must capture at least e.g. 60% of patients who will become addicted\n",
    "\n",
    "Among thresholds that satisfy recall ≥ 0.6, you then pick the one with the best precision, to minimize unnecessary undertreatment and workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af300cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: choose threshold by recall floor\n",
    "RECALL_FLOOR = 0.60 # <- judgment call\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "THR = float(res[\"threshold\"])\n",
    "\n",
    "# Step 2: numeric summary at the chosen threshold\n",
    "summary_df = summary_at_threshold(y_val, y_score_val, THR)\n",
    "display(summary_df.round(3).set_index(\"threshold\"))\n",
    "\n",
    "# Step 3: precision and recall vs threshold with annotations\n",
    "plot_recall_floor_curves(y_val, y_score_val, recall_floor=RECALL_FLOOR, chosen_threshold=THR)\n",
    "\n",
    "# Step 4: cumulative recall vs alerts with vertical line at alerts implied by THR\n",
    "plot_cumulative_recall_at_threshold(y_val, y_score_val, chosen_threshold=THR)\n",
    "\n",
    "# Step 5: patient-level prioritization view at THR\n",
    "plot_topk_at_threshold(y_val, y_score_val, chosen_threshold=THR, top_k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48885c",
   "metadata": {},
   "source": [
    "### 3. Cost based threshold for calibrated_clf on validation\n",
    "\n",
    "This approach selects a threshold that minimizes the expected cost of errors. We need to estimate the relative costs of false negatives (Givin opioid drugs to people who would actually get addited) and false positives (Depriving low risk people from receiving the medicine, costs in other terapies, etc.).\n",
    "\n",
    "- Cost of a False Negative (C_FN): The cost of missing a patient who will develop an opioid use disorder (e.g., costs of future intensive treatment, overdose events, and negative health outcomes, social impact, etc.)\n",
    "\n",
    "- Cost of a False Positive (C_FP): The cost of incorrectly flagging a patient as high-risk (e.g., costs of unnecessary clinical review, potential undertreatment of legitimate pain, and patient anxiety)\n",
    "\n",
    "For this exercise, we will assume a cost ratio where a FN is 10 times more costly than a false positive. The optimal threshold is then found by minimizing the total expected cost (C_FP * FP + C_FN * FN) on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set domain costs\n",
    "C_FN = 100.0  # Cost of a missed addiction case (False Negative)\n",
    "C_FP = 10.0   # Cost of undertreated pain (False Positive)\n",
    "\n",
    "# Get scores from the calibrated model on the validation set\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Find the optimal thresholds based on cost\n",
    "res = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "tbl = res[\"table\"].copy()\n",
    "tbl[\"expected_cost\"] = C_FP * tbl[\"FP\"] + C_FN * tbl[\"FN\"]\n",
    "\n",
    "# Plot expected cost vs. threshold\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"expected_cost\"])\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Expected Cost\")\n",
    "plt.title(\"Cost-Based Threshold Selection (Validation Set)\")\n",
    "\n",
    "# Annotate the two optimal thresholds found\n",
    "t_formula = res[\"threshold_formula\"]\n",
    "t_emp = res[\"threshold_empirical\"]\n",
    "plt.axvline(t_formula, linestyle=\"--\", color=\"red\", label=f\"Bayes Optimal Thr={t_formula:.2f}\")\n",
    "plt.axvline(t_emp, linestyle=\":\", color=\"black\", label=f\"Empirical Min Thr={t_emp:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display a summary table comparing the two threshold options\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "display(summary_df.round(3).set_index(\"rule\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81633a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the empirically optimal threshold from the previous analysis\n",
    "THR = res[\"threshold_empirical\"]\n",
    "\n",
    "# For visualization, we need a \"recall_floor\" to draw the horizontal line.\n",
    "# We'll use the actual recall achieved at our chosen cost-based threshold.\n",
    "y_pred_val = (y_score_val >= THR).astype(int)\n",
    "recall_at_thr = recall_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Visualizing performance at the empirical cost-based threshold of {THR:.3f}:\")\n",
    "plot_recall_floor_curves(y_val, y_score_val, recall_floor=recall_at_thr, chosen_threshold=THR)\n",
    "plot_cumulative_recall_at_threshold(y_val, y_score_val, chosen_threshold=THR)\n",
    "plot_topk_at_threshold(y_val, y_score_val, chosen_threshold=THR, top_k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e712ce",
   "metadata": {},
   "source": [
    "### Final Threshold Selection and Test Set Evaluation\n",
    "\n",
    "This is a crucial step that often involves discussion with clinical stakeholders to align the model's operating point with clinical goals, patient safety requirements, and resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8947a6",
   "metadata": {},
   "source": [
    "| Thresholding Method | Objective | Pros (Best for...) | Cons (Potential Risks) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1. Workload Constrained** | Maximize TP (addiction risk) under a fixed (alert) budget | Good for limited resources, ensuring Ops stability | A low budget will miss many high-risk patients |\n",
    "| **2. Recall Floor** | Maximize precision (TP vs. FP) while holding a minimum recall rate (breath) | Acts as a safety net, ensuring a minimum capture rate | Increases false alarms e.g. workloads from alternative treatments, untreated pain |\n",
    "| **3. Cost-Based** | Minimize total expected cost by assigning costs to errors | Provides a formal f/ work to balance different clinical errors | Highly dependent on accurate cost estimates (can be subjective) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be2149",
   "metadata": {},
   "source": [
    "1. Compute the candidate threshold (THR) for each policy on the validation set\n",
    "2. Compare them on consistent operational metrics and expected cost\n",
    "3. Lock the final threshold using the same (THR) variable, to get an unbiased estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da659da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds are derived on validation, comparison is on test\n",
    "\n",
    "# Scores\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "\n",
    "# Derive thresholds on validation only, for the three methods\n",
    "res_rec  = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "thr_rec  = float(res_rec[\"threshold\"])\n",
    "\n",
    "res_work = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "thr_work = float(res_work[\"threshold\"])\n",
    "\n",
    "res_cost = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "thr_cost_emp = float(res_cost[\"threshold_empirical\"])\n",
    "\n",
    "# Compare policies on the unseen test set using the frozen (validation) thresholds\n",
    "rows = []\n",
    "for approach, thr in [\n",
    "    (\"1. Workload budget\", thr_work),\n",
    "    (\"2. Recall floor\",    thr_rec),\n",
    "    (\"3. Cost min emp\",    thr_cost_emp),\n",
    "]:\n",
    "    s = summary_at_threshold(y_test, y_score_test, thr).iloc[0].to_dict()\n",
    "    s.update({\n",
    "        \"approach\": approach,\n",
    "        \"expected_cost\": float(C_FP * s[\"FP\"] + C_FN * s[\"FN\"]),\n",
    "    })\n",
    "    rows.append(s)\n",
    "\n",
    "test_compare = (\n",
    "    pd.DataFrame(rows)[\n",
    "        [\"approach\", \"threshold\", \"precision\", \"recall\",\n",
    "         \"alerts_per_1000\", \"true_pos_per_1000\", \"TP\", \"FP\", \"FN\", \"expected_cost\"]\n",
    "    ]\n",
    "    .sort_values(\"approach\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(test_compare.round(3))\n",
    "\n",
    "# Also report threshold-independent curve metrics on test for context\n",
    "test_roc_auc = float(roc_auc_score(y_test, y_score_test))\n",
    "test_pr_auc  = float(average_precision_score(y_test, y_score_test))\n",
    "print(f\"Test ROC AUC={test_roc_auc:.3f}  |  Test PR AUC={test_pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e9541",
   "metadata": {},
   "source": [
    "### Choose the final threshold explicitly, then evaluate on test\n",
    "Manually lock the final threshold using one of the frozen candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65540e10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed882ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment exactly one line below to choose the policy used in class\n",
    "\n",
    "# THR = thr_rec        # Recall floor\n",
    "# THR = thr_work       # Workload budget\n",
    "# THR = thr_cost_emp   # Cost minimization (empirical)\n",
    "\n",
    "assert \"THR\" in globals(), \"Set THR by uncommenting one of the lines above\"\n",
    "\n",
    "# Final unbiased evaluation on the test set\n",
    "final_summary = summary_at_threshold(y_test, y_score_test, THR).round(3)\n",
    "display(final_summary.set_index(\"threshold\"))\n",
    "\n",
    "# Expose for downstream use\n",
    "proba_test = y_score_test\n",
    "pred_test = (y_score_test >= THR).astype(int)\n",
    "\n",
    "# Minimal metrics dict for saving or later reporting\n",
    "metrics = {\n",
    "    \"policy_threshold\": float(THR),\n",
    "    \"test_precision\": float(final_summary[\"precision\"].iloc[0]),\n",
    "    \"test_recall\": float(final_summary[\"recall\"].iloc[0]),\n",
    "    \"test_TP\": int(final_summary[\"TP\"].iloc[0]),\n",
    "    \"test_FP\": int(final_summary[\"FP\"].iloc[0]),\n",
    "    \"test_FN\": int(final_summary[\"FN\"].iloc[0]),\n",
    "    \"test_alerts_per_1000\": float(final_summary[\"alerts_per_1000\"].iloc[0]),\n",
    "    \"test_true_pos_per_1000\": float(final_summary[\"true_pos_per_1000\"].iloc[0]),\n",
    "    \"test_roc_auc\": test_roc_auc,\n",
    "    \"test_pr_auc\": test_pr_auc,\n",
    "}\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581b38e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ce4a28b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebed8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_rai_lgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
