{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b54679",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook is part of a machine learning for healthcare exercise, focusing on using the Responsible AI (RAI) package to enhance clinical decision-making. The toolkit will be used to analyze opioid use disorder (OD) risk, with three key objectives:\n",
    "\n",
    "1. Analyze Errors and Explore Interpretability of Models: We will run Interpret-Community\u2019s global explainers to generate feature importance insights and visualize model errors with the Error Analysis dashboard\n",
    "\n",
    "2. Plan real-world action through counterfactual and causal analysis: By leveraging counterfactual examples and causal inference, we will explore decision-making strategies based on opioid prescription patterns and patient comorbidities to understand possible interventions and their impacts\n",
    "\n",
    "3. Assess addiction risk predictions: A classification model trained on patient-level features (income, surgeries, opioid prescription days, and comorbidities A\u2013V) will be evaluated to examine its performance in predicting risk of opioid use disorder and to inform prevention strategies\n",
    "\n",
    "**The goal is to provide non-trivial insights for clinical decision making, leveraging machine learning paired with responsible AI tools, to improve patient outcomes in the healthcare context.**\n",
    "\n",
    "Based on notebooks and documentation from the [Responsible AI toolkit](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934c151",
   "metadata": {},
   "source": [
    "# 2. Data Set Characteristics\n",
    "\n",
    "Number of Instances: patient-level records (rows)\n",
    "\n",
    "Number of Attributes: 20 predictive attributes and 1 target class\n",
    "\n",
    "Attribute Information:\n",
    "- OD (target): whether the patient had an opioid use disorder diagnosis (binary: 1 = yes, 0 = no)\n",
    "- Low_inc: low income flag (1 = low income, 0 = not low income)\n",
    "- Surgery: whether the patient underwent major surgery in the 2 years\n",
    "- rx_ds: number of days of prescribed opioids in the 2 years\n",
    "- A: infectious diseases group A (binary flag)\n",
    "- B: infectious diseases group B\n",
    "- C: malignant neoplasm\n",
    "- D: benign neoplasm\n",
    "- E: endocrine conditions\n",
    "- F: mental and behavioral health conditions (excluding opioid-related)\n",
    "- H: ear conditions\n",
    "- I: circulatory system conditions\n",
    "- J: respiratory system conditions\n",
    "- K: digestive system conditions\n",
    "- L: skin conditions\n",
    "- M: musculoskeletal system conditions\n",
    "- N: urinary system conditions\n",
    "- R: other signs and symptoms\n",
    "- S: injuries\n",
    "- T: burns and toxic conditions\n",
    "- V: external trauma conditions\n",
    "\n",
    "class:\n",
    "- OD = 1: patient identified with opioid use disorder in the 2 years\n",
    "- OD = 0: patient without opioid use disorder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da36d3",
   "metadata": {},
   "source": [
    "# 3. Setup\n",
    "- responsibleai and raiwidgets provide RAIInsights and the dashboard\n",
    "- fairlearn provides fairness metrics and mitigation algorithms used under the hood\n",
    "- imbalanced-learn offers resampling utilities if you want to experiment with imbalance mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f488354a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RAI core/widgets unavailable: No module named 'responsibleai'\nInterpret-Community unavailable: No module named 'interpret_community'\nError Analysis unavailable: No module named 'erroranalysis'\nFairlearn metrics unavailable: No module named 'fairlearn'\n_RAI=False, _INTERPRET=False, _ERRANALYSIS=False, _FAIRLEARN=False, _DICE=False, _ECONML=False\n"
    }
   ],
   "source": [
    "# Init placeholders for later RAI prep\n",
    "train_df_rai = None\n",
    "test_df_rai = None\n",
    "feature_metadata = None\n",
    "\n",
    "# Core libs\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    log_loss,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# Optional extras\n",
    "try:\n",
    "    import dice_ml  # counterfactual library\n",
    "    _DICE = True\n",
    "except Exception:\n",
    "    _DICE = False\n",
    "\n",
    "try:\n",
    "    import econml  # causal library\n",
    "    _ECONML = True\n",
    "except Exception:\n",
    "    _ECONML = False\n",
    "\n",
    "# Course utilities\n",
    "from utils import (\n",
    "    positive_scores,\n",
    "    auc_report,\n",
    "    tradeoff_table,\n",
    "    pick_threshold_cost,\n",
    "    pick_threshold_recall_floor,\n",
    "    pick_threshold_workload,\n",
    "    summary_at_threshold,\n",
    "    plot_recall_floor_curves,\n",
    "    plot_cumulative_recall_at_threshold,\n",
    "    plot_topk_at_threshold,\n",
    "    make_thresholded_estimator,\n",
    "    init_rai_dependencies,\n",
    ")\n",
    "\n",
    "# RAI dependency detection\n",
    "rai_flags, rai_objects = init_rai_dependencies()\n",
    "globals().update(rai_flags)\n",
    "globals().update(rai_objects)\n",
    "\n",
    "# Repro seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Quick status\n",
    "print(f\"_RAI={_RAI}, _INTERPRET={_INTERPRET}, _ERRANALYSIS={_ERRANALYSIS}, _FAIRLEARN={_FAIRLEARN}, _DICE={_DICE}, _ECONML={_ECONML}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc05cf",
   "metadata": {},
   "source": [
    "# 4. Data Load & preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f881f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV (update if needed for your system)\n",
    "DATA_PATH = \"./Data/opiod_raw_data.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome prevalence and missingness\n",
    "\n",
    "TARGET = \"OD\"  # target: 1 = opioid use disorder diagnosis, 0 = none\n",
    "\n",
    "# Outcome prevalence\n",
    "counts = df_raw[TARGET].value_counts(dropna=False)\n",
    "prevalence_percent = counts[1] / counts.sum() * 100\n",
    "positives_per_1000 = counts[1] / counts.sum() * 1000\n",
    "\n",
    "print(\"Outcome counts:\")\n",
    "print(counts)\n",
    "print(f\"\\nPrevalence: {prevalence_percent:.2f}%\")\n",
    "print(f\"Patients with OD per 1000: {positives_per_1000:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142570c9",
   "metadata": {},
   "source": [
    "## 4.1. Basic cleaning and schema alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9316e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the raw DataFrame\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [c.strip().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "              for c in df.columns]\n",
    "\n",
    "# Drop ID column\n",
    "if df.shape[1] > 0:\n",
    "    df = df.drop(columns=[df.columns[0]])\n",
    "\n",
    "# Harmonize known aliases\n",
    "if \"SURG\" in df.columns and \"Surgery\" not in df.columns:\n",
    "    df = df.rename(columns={\"SURG\": \"Surgery\"})\n",
    "\n",
    "# Expected columns from the data dictionary\n",
    "expected_cols = [\n",
    "    \"OD\", \"Low_inc\", \"Surgery\", \"rx_ds\",\n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"R\", \"S\", \"T\", \"V\"\n",
    "]\n",
    "\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# Helper to coerce various binary encodings to 0/1\n",
    "\n",
    "\n",
    "def to_binary(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype == \"O\":\n",
    "        mapped = s.astype(str).str.strip().str.lower().map({\n",
    "            \"1\": 1, \"0\": 0,\n",
    "            \"y\": 1, \"n\": 0,\n",
    "            \"yes\": 1, \"no\": 0,\n",
    "            \"true\": 1, \"false\": 0\n",
    "        })\n",
    "        s = pd.to_numeric(mapped, errors=\"coerce\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34345072",
   "metadata": {},
   "source": [
    "# 5. Training, Validation and Testing - 80/15/5 stratified split\n",
    "\n",
    "Train\u2013test split\n",
    "\n",
    "- Prevents \u201cpeeking\u201d at data and overestimating performance\n",
    "- Mimics real-world deployment where models face unseen patients\n",
    "- Always evaluate on data not used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"OD\"\n",
    "num_cols = [\"rx_ds\"]\n",
    "\n",
    "# Infer binary columns: columns with only 0/1 values, excluding target and numeric columns\n",
    "binary_cols = [col for col in df.columns\n",
    "               if col not in [target_col] + num_cols\n",
    "               and set(df[col].dropna().unique()).issubset({0, 1})]\n",
    "\n",
    "cat_like_binary_cols = binary_cols.copy()\n",
    "\n",
    "X = df[num_cols + cat_like_binary_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Split temp into validation and test, 15% each overall\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")\n",
    "# Display shapes and class distributions as a DataFrame\n",
    "\n",
    "info = {\n",
    "    \"Set\": [\"Train\", \"Validation\", \"Test\", \"Overall\"],\n",
    "    \"X shape\": [X_train.shape, X_val.shape, X_test.shape, X.shape],\n",
    "    \"y shape\": [y_train.shape, y_val.shape, y_test.shape, y.shape],\n",
    "    \"p(OD=0)\": [\n",
    "        y_train.value_counts(normalize=True).get(0, 0.0),\n",
    "        y_val.value_counts(normalize=True).get(0, 0.0),\n",
    "        y_test.value_counts(normalize=True).get(0, 0.0),\n",
    "        y.value_counts(normalize=True).get(0, 0.0),\n",
    "    ],\n",
    "    \"p(OD=1)\": [\n",
    "        y_train.value_counts(normalize=True).get(1, 0.0),\n",
    "        y_val.value_counts(normalize=True).get(1, 0.0),\n",
    "        y_test.value_counts(normalize=True).get(1, 0.0),\n",
    "        y.value_counts(normalize=True).get(1, 0.0),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_info = pd.DataFrame(info)\n",
    "display(df_info.style.format({\"p(OD=0)\": \"{:.3f}\", \"p(OD=1)\": \"{:.3f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0a7ff",
   "metadata": {},
   "source": [
    "## 5.1. Modeling pipeline, training, and calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63d7b7",
   "metadata": {},
   "source": [
    "### Setting a baseline\n",
    "A naive majority-class baseline clarifies the minimum standard any model must beat, highlighting the danger of ignoring minority patients and ensuring improvements carry meaningful weight in healthcare decision making\n",
    "\n",
    "**ROC AUC**  \n",
    "- 0.5 \u2192 no discrimination\n",
    "- 0.6\u20130.7 \u2192 poor\n",
    "- 0.7\u20130.8 \u2192 fair\n",
    "- 0.8\u20130.9 \u2192 good\n",
    "- \u2265 0.9 \u2192 excellent\n",
    "\n",
    "**PR AUC**  \n",
    "- Must be interpreted against event prevalence `p` in the validation set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: DummyClassifier (majority class)\n",
    "dummy_clf = DummyClassifier(\n",
    "    strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "proba_val = dummy_clf.predict_proba(X_val)\n",
    "pos_idx = int(np.where(dummy_clf.classes_ == 1)[0][0])  # index for class \"1\"\n",
    "y_score_val = np.asarray(proba_val)[:, pos_idx]\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(dummy_clf, X_val)\n",
    "metrics_dummy = auc_report(\n",
    "    y_val, y_score_val, name=\"Dummy baseline\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bc2a2",
   "metadata": {},
   "source": [
    "## 5.2. Preprocesing our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74d963",
   "metadata": {},
   "source": [
    "[Scikit-learn preprocessing](https://scikit-learn.org/stable/api/sklearn.preprocessing.html) standardizes and transforms features for modeling, including scaling, encoding, and imputation. Helping to maintain a consistent data transformation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True))\n",
    "])\n",
    "\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"bin\", binary_transformer, cat_like_binary_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",  # drops any column not in previously specified\n",
    "    verbose_feature_names_out=False  # keeps original feature names\n",
    ")\n",
    "\n",
    "# Logistic Regression baseline with variance filter\n",
    "base_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    # Eliminates features with zero variance\n",
    "    (\"varth\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"liblinear\",  # See details in course material\n",
    "        class_weight=\"balanced\",  # Adjusts weights for class imbalance\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=200\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8ccc7",
   "metadata": {},
   "source": [
    "### Basic preliminary model performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit base model\n",
    "base_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(base_clf, X_val)\n",
    "metrics_base = auc_report(y_val, y_score_val, name=\"base_clf\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4634f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five patients: raw pre-calibration score, predicted label, actual label\n",
    "\n",
    "tbl5 = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"raw_score\": positive_scores(base_clf, X_val),\n",
    "            \"actual\": y_val.loc[X_val.index].astype(int),\n",
    "        },\n",
    "        index=X_val.index,\n",
    "    )\n",
    "    .assign(predicted=lambda d: (d[\"raw_score\"] >= 0.50).astype(int))  # threshold before calibration\n",
    "    .round({\"raw_score\": 3})\n",
    "    .sample(n=10, random_state=7)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"patient\"})\n",
    "    .loc[:, [\"patient\", \"raw_score\", \"predicted\", \"actual\"]]\n",
    ")\n",
    "\n",
    "display(tbl5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the plot using variables from the preceding cells\n",
    "scores = positive_scores(base_clf, X_val)\n",
    "actuals = y_val\n",
    "\n",
    "# Separate scores for actual positive (OD=1) and negative (OD=0) cases\n",
    "scores_positive = scores[actuals == 1]\n",
    "scores_negative = scores[actuals == 0]\n",
    "\n",
    "# Generate random jitter for the x-axis\n",
    "jitter_strength = 0.03\n",
    "jitter_positive = np.random.normal(0, jitter_strength, len(scores_positive))\n",
    "jitter_negative = np.random.normal(0, jitter_strength, len(scores_negative))\n",
    "\n",
    "# 3. Create the plot\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "scatter_other = ax.scatter(jitter_negative, scores_negative, color='darkslategrey', alpha=0.3, label='Other')\n",
    "scatter_target = ax.scatter(jitter_positive, scores_positive, color='red', alpha=0.7, label='OD')\n",
    "\n",
    "# Add threshold lines\n",
    "line_50 = ax.axhline(y=0.5, color='red', linestyle='-', linewidth=2, label='Threshold = 0.5')\n",
    "line_80 = ax.axhline(y=0.7, color='orange', linestyle='--', linewidth=2, label='Threshold = 0.7')\n",
    "\n",
    "ax.set_title('Scores vs. Actuals')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_xlim(-0.2, 0.2) # Set fixed x-axis limits to control the visual spread\n",
    "ax.legend(handles=[scatter_target, scatter_other, line_50, line_80], fontsize=9, loc='upper right')\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e413ac1",
   "metadata": {},
   "source": [
    "# 6. Recalibrating the Scores\n",
    "\n",
    "**1. Reliable probabilities**  \n",
    "- Turns raw scores into real probabilities  \n",
    "- Ensures predictions match observed outcome frequencies  \n",
    "- Prevents overly high or low risk estimates  \n",
    "\n",
    "**2. Better clinical decisions**  \n",
    "- Essential when risk values guide medical choices  \n",
    "- Supports thresholds with clear clinical meaning\n",
    "- Reduces wasted clinical resources  \n",
    "\n",
    "**3. Trust and adoption**  \n",
    "- Builds trust in AI decisions  \n",
    "- Enables safer patient outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate probabilities using CV on training data\n",
    "calibrated_clf = CalibratedClassifierCV(\n",
    "    estimator=base_clf,\n",
    "    method=\"sigmoid\",\n",
    "    cv=5\n",
    ")\n",
    "calibrated_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile bands on test only\n",
    "base_clf_uncal = clone(base_clf).fit(X_train, y_train)\n",
    "p_before = base_clf_uncal.predict_proba(X_test)[:, 1]\n",
    "p_after = calibrated_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_cal = pd.DataFrame({\"Actual_OD\": y_test.values, \"Pred_before\": p_before,\n",
    "                      \"Pred_after\": p_after}, index=X_test.index)\n",
    "\n",
    "# 10 quantile bands with similar counts\n",
    "df_cal[\"Risk_band\"] = pd.qcut(df_cal[\"Pred_after\"], q=10, labels=[\n",
    "                              f\"Q{i}\" for i in range(1, 11)], duplicates=\"drop\")\n",
    "\n",
    "summary = (\n",
    "    df_cal.groupby(\"Risk_band\", observed=True)\n",
    "    .agg(Patients=(\"Actual_OD\", \"size\"),\n",
    "         Avg_pred_before=(\"Pred_before\", \"mean\"),\n",
    "         Actual_OD_rate=(\"Actual_OD\", \"mean\"),\n",
    "         Avg_pred_after=(\"Pred_after\", \"mean\"))\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "print(\"Check sizes\")\n",
    "print(\"len(X_train) =\", len(X_train), \"len(X_val) =\",\n",
    "      len(X_val), \"len(X_test) =\", len(X_test))\n",
    "print(\"Rows in table sum to\", int(summary[\"Patients\"].sum()))\n",
    "\n",
    "# Reliability plot using the same fixed risk bands summary\n",
    "print(\"Points near the diagonal mean predicted risk matches observed OD frequency\")\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\",\n",
    "         linewidth=1, label=\"Perfectly calibrated\")\n",
    "plt.scatter(summary[\"Avg_pred_before\"],\n",
    "            summary[\"Actual_OD_rate\"], label=\"Before calibration\")\n",
    "plt.scatter(summary[\"Avg_pred_after\"],\n",
    "            summary[\"Actual_OD_rate\"],  label=\"After calibration\")\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Observed OD rate\")\n",
    "plt.title(\"Calibration reliability by risk bands\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234181a1",
   "metadata": {},
   "source": [
    "### Compare base_clf vs calibrated_clf on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d490f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on discrimination equality and calibration improvements\n",
    "\n",
    "# Scores\n",
    "y_score_val_base = positive_scores(base_clf, X_val)\n",
    "y_score_val_cal = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Discrimination\n",
    "roc_base = roc_auc_score(y_val, y_score_val_base)\n",
    "pr_base = average_precision_score(y_val, y_score_val_base)\n",
    "roc_cal = roc_auc_score(y_val, y_score_val_cal)\n",
    "pr_cal = average_precision_score(y_val, y_score_val_cal)\n",
    "\n",
    "# Calibration\n",
    "ll_base = log_loss(y_val, np.clip(y_score_val_base, 1e-6, 1 - 1e-6))\n",
    "ll_cal = log_loss(y_val, np.clip(y_score_val_cal,  1e-6, 1 - 1e-6))\n",
    "brier_base = brier_score_loss(y_val, y_score_val_base)\n",
    "brier_cal = brier_score_loss(y_val, y_score_val_cal)\n",
    "\n",
    "# Assemble into dataframe\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"Metric\": \"ROC AUC\", \"Base\": roc_base, \"Calibrated\": roc_cal, \"Explanation\": \"Ability to distinguish between classes (higher is better)\"},\n",
    "    {\"Metric\": \"PR AUC\", \"Base\": pr_base, \"Calibrated\": pr_cal, \"Explanation\": \"Precision-recall curve area; useful for imbalanced data\"},\n",
    "    {\"Metric\": \"Log loss\", \"Base\": ll_base, \"Calibrated\": ll_cal, \"Explanation\": \"Penalty for incorrect and overconfident predictions (lower is better)\"},\n",
    "    {\"Metric\": \"Brier score\", \"Base\": brier_base, \"Calibrated\": brier_cal, \"Explanation\": \"Mean squared error of predicted probabilities (lower is better)\"},\n",
    "])\n",
    "\n",
    "display(metrics_df.style.format({\"Base\": \"{:.3f}\", \"Calibrated\": \"{:.3f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c73af",
   "metadata": {},
   "source": [
    "# 7. Deciding where to cut off i.e. what probability is \u201chigh risk enough\u201d to trigger an intervention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96638f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get calibrated probabilities\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Define a grid of thresholds\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "rows = []\n",
    "for thr in thresholds:\n",
    "    y_pred = (y_score_val >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    alerts_per_1000 = 1000 * np.mean(y_pred)\n",
    "    true_pos_per_1000 = 1000 * tp / len(y_val)\n",
    "    rows.append({\n",
    "        \"threshold\": thr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"alerts_per_1000\": alerts_per_1000,\n",
    "        \"true_pos_per_1000\": true_pos_per_1000,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "# Display a few candidate thresholds for discussion\n",
    "display(\n",
    "    df_thr.query(\"threshold in [0.1, 0.2, 0.3, 0.4, 0.5]\")\n",
    "         .round(3)\n",
    "         .set_index(\"threshold\")\n",
    ")\n",
    "\n",
    "# Plot workload vs threshold\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000 patients\")\n",
    "plt.title(\"Operational tradeoffs vs threshold (validation set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cd2f5",
   "metadata": {},
   "source": [
    "## 7.1. Choosing an operating threshold\n",
    "\n",
    "Models produce probabilities, but clinicians are the ones taking decisions, and carring the accountability of their actions.\n",
    "\n",
    "- Setting a threshold balances in this case, among others, between missed addiction cases and unnecessary undertreatment of pain\n",
    "- Clear rules make these tradeoffs explicit, explainable, and auditable!\n",
    "\n",
    "We will run three threshold tuning analyses:\n",
    "1. **Workload constrained threshold**  \n",
    "  Capture the most true cases without exceeding a fixed alert capacity for the clinic\n",
    "2. **Recall floor then maximize precision**  \n",
    "  Guarantee a minimum case capture for safety, then pick the threshold with the fewest false alarms\n",
    "3. **Cost based threshold (Bayes rule)**  \n",
    "  Minimize expected harm using estimated costs for false negatives and false positives\n",
    "\n",
    "Readouts to watch\n",
    "- Threshold, precision, recall, alerts per 1000 patients, true positives per 1000, false positives, false negatives\n",
    "- Connect the chosen rule to clinical policy and resource capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f08220",
   "metadata": {},
   "source": [
    "### 7.1.1. Workload constrained threshold for calibrated_clf on validation\n",
    "\n",
    "- Alerts budget: maximum alerts per 1000 patients the clinic can review without overloading resources\n",
    "- Objective: within the alerts budget, choose the threshold that yields the most true positives per 1000 so more patients at real risk are correctly flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALERTS_BUDGET = 100.0  # alerts per 1000 patients\n",
    "\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "tbl = res[\"table\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.axhline(ALERTS_BUDGET, linestyle=\"--\")\n",
    "plt.axvline(res[\"threshold\"], linestyle=\":\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000\")\n",
    "plt.title(\"Workload constrained threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# make sure summary_df is a DataFrame\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "\n",
    "# put metrics in the rows, rules in the columns\n",
    "pivot_df = summary_df.set_index(\"rule\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0f3a8",
   "metadata": {},
   "source": [
    "Let's now see the risk groups by decile (bands), and visualizes the overall risk score distribution, with optional threshold overlay and decile boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BINS = 10  # deciles by default\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Build bands by quantiles, highest risk = band 1\n",
    "bands = pd.qcut(y_score_val, q=N_BINS, labels=False, duplicates=\"drop\")\n",
    "# qcut labels lowest=0..highest=K-1, invert so 1 is highest-risk band\n",
    "bands = (bands.max() - bands) + 1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"y_true\": y_val.astype(int),\n",
    "    \"score\": y_score_val,\n",
    "    \"band\": bands.astype(int),\n",
    "})\n",
    "\n",
    "summ = (df.groupby(\"band\", as_index=True)\n",
    "          .agg(n=(\"y_true\",\"size\"),\n",
    "               positives=(\"y_true\",\"sum\"),\n",
    "               min_score=(\"score\",\"min\"),\n",
    "               max_score=(\"score\",\"max\"))\n",
    "          .sort_index())\n",
    "\n",
    "summ[\"prevalence\"] = summ[\"positives\"] / summ[\"n\"]\n",
    "summ[\"cum_capture\"] = summ[\"positives\"].cumsum() / df[\"y_true\"].sum()\n",
    "summ[\"alerts_per_1000\"] = 1000.0 * summ[\"n\"] / len(df)\n",
    "summ[\"true_pos_per_1000\"] = 1000.0 * summ[\"positives\"] / len(df)\n",
    "\n",
    "display(summ.round(3))\n",
    "\n",
    "# Optional threshold overlay if you already chose one, else set THR=None\n",
    "THR = None  # e.g., THR = 0.23\n",
    "\n",
    "# Histogram of risk scores with decile edges\n",
    "plt.figure()\n",
    "plt.hist(df[\"score\"], bins=30)\n",
    "if THR is not None:\n",
    "    plt.axvline(THR, linestyle=\"--\")\n",
    "# draw decile boundaries\n",
    "edges = np.quantile(df[\"score\"], np.linspace(0,1,N_BINS+1))\n",
    "for x in edges:\n",
    "    plt.axvline(x, linestyle=\":\", linewidth=0.8)\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Patients\")\n",
    "plt.title(\"Risk distribution with decile boundaries\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "THR = res[\"threshold\"] # <- use the threshold from the previous step\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "ids = np.arange(len(y_val))\n",
    "\n",
    "# Sort patients by predicted risk\n",
    "order = np.argsort(-y_score_val)\n",
    "top_idx = order[:30]   # top 30 for visualization\n",
    "top_scores = y_score_val[top_idx]\n",
    "top_true = np.asarray(y_val)[top_idx].astype(int)\n",
    "\n",
    "# Split indices for TP vs FP\n",
    "tp_idx = np.where(top_true == 1)[0]\n",
    "fp_idx = np.where(top_true == 0)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(tp_idx, top_scores[tp_idx], color=\"tab:red\", label=\"True addicted (TP)\")\n",
    "plt.bar(fp_idx, top_scores[fp_idx], color=\"tab:gray\", label=\"Not addicted (FP)\")\n",
    "plt.axhline(THR, linestyle=\"--\", color=\"black\", label=f\"Threshold = {THR:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Patients ranked by predicted risk\")\n",
    "plt.ylabel(\"Predicted risk\")\n",
    "plt.title(\"Top 30 highest-risk patients on validation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195459c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "order = np.argsort(-y_score_val)\n",
    "y_sorted = np.asarray(y_val)[order].astype(int)\n",
    "\n",
    "# Cumulative recall\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "total_pos = cum_tp[-1] if cum_tp.size else 0\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "recall_curve = cum_tp / total_pos if total_pos > 0 else np.zeros_like(cum_tp)\n",
    "\n",
    "# Budget for alerts\n",
    "n_budget = int(np.ceil(ALERTS_BUDGET * len(y_val) / 1000.0))\n",
    "\n",
    "# Recall at budget\n",
    "recall_at_budget = recall_curve[n_budget - 1] if n_budget > 0 and n_budget <= len(y_val) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, recall_curve, label=\"Cumulative recall\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", color=\"red\", label=f\"Budget = {n_budget} alerts\")\n",
    "\n",
    "# Annotate recall at budget\n",
    "plt.scatter(n_budget, recall_at_budget, color=\"black\", zorder=5)\n",
    "plt.text(n_budget + 2, recall_at_budget, f\"Recall = {recall_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Cumulative capture of true cases vs alerts\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative precision at top-k alerts\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "precision_curve = cum_tp / alerts\n",
    "\n",
    "# Alerts budget scaled to validation size\n",
    "prec_at_budget = precision_curve[n_budget - 1] if 0 < n_budget <= len(y_sorted) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, precision_curve, label=\"Precision at top-k alerts\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", label=f\"Budget = {n_budget} alerts\")\n",
    "plt.scatter(n_budget, prec_at_budget, zorder=5)\n",
    "plt.text(n_budget + max(2, len(y_sorted)//100), prec_at_budget, f\"Precision = {prec_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs number of alerts\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c8fbb",
   "metadata": {},
   "source": [
    "### 7.1.2. Recall floor then maximize precision for calibrated_clf on validation\n",
    "\n",
    "- Recall floor: minimum acceptable recall set by safety policy to limit missed addiction cases\n",
    "- Precision objective: among thresholds meeting the recall floor, pick the one with highest precision to reduce unnecessary undertreatment and clinician workload\n",
    "\n",
    "Deciding which recall floor to sue:\n",
    "1. The chosen floor is a value judgment balancing patient safety vs resource burden\n",
    "2. In medicine, it's often the case that missing a true case (false negative) is often much worse than raising extra alarms (false positives)\n",
    "3. A recall floor enforces a safety guarantee: the model must capture at least e.g. 60% of patients who will become addicted\n",
    "\n",
    "Among thresholds that satisfy recall \u2265 0.6, you then pick the one with the best precision, to minimize unnecessary undertreatment and workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af300cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: choose threshold by recall floor\n",
    "RECALL_FLOOR = 0.60 # <- judgment call\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "THR = float(res[\"threshold\"])\n",
    "\n",
    "summary_df = summary_at_threshold(y_val, y_score_val, THR)\n",
    "pivot_df = summary_df.set_index(\"threshold\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# Precision and recall vs threshold with annotations\n",
    "plot_recall_floor_curves(y_val, y_score_val, recall_floor=RECALL_FLOOR, chosen_threshold=THR)\n",
    "\n",
    "# Cumulative recall vs alerts with vertical line at alerts implied by THR\n",
    "plot_cumulative_recall_at_threshold(y_val, y_score_val, chosen_threshold=THR)\n",
    "\n",
    "# Patient-level prioritization view at THR\n",
    "plot_topk_at_threshold(y_val, y_score_val, chosen_threshold=THR, top_k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48885c",
   "metadata": {},
   "source": [
    "### 7.1.3. Cost based threshold for calibrated_clf on validation\n",
    "\n",
    "This approach selects a threshold that minimizes the expected cost of errors. We need to estimate the relative costs of false negatives (Givin opioid drugs to people who would actually get addited) and false positives (Depriving low risk people from receiving the medicine, costs in other terapies, etc.).\n",
    "\n",
    "- Cost of a False Negative (C_FN): The cost of missing a patient who will develop an opioid use disorder (e.g., costs of future intensive treatment, overdose events, and negative health outcomes, social impact, etc.)\n",
    "\n",
    "- Cost of a False Positive (C_FP): The cost of incorrectly flagging a patient as high-risk (e.g., costs of unnecessary clinical review, potential undertreatment of legitimate pain, and patient anxiety)\n",
    "\n",
    "For this exercise, we will assume a cost ratio where a FN is 10 times more costly than a false positive. The optimal threshold is then found by minimizing the total expected cost (C_FP * FP + C_FN * FN) on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set domain costs\n",
    "C_FN = 100.0  # Cost of a missed addiction case (False Negative)\n",
    "C_FP = 10.0   # Cost of undertreated pain (False Positive)\n",
    "\n",
    "# Get scores from the calibrated model on the validation set\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Find the optimal thresholds based on cost\n",
    "res = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "tbl = res[\"table\"].copy()\n",
    "tbl[\"expected_cost\"] = C_FP * tbl[\"FP\"] + C_FN * tbl[\"FN\"]\n",
    "\n",
    "# Display a summary table comparing the two threshold options\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "\n",
    "pivot_df = (\n",
    "    summary_df.set_index(\"rule\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    ")\n",
    "\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# Plot expected cost vs. threshold\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"expected_cost\"])\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Expected Cost\")\n",
    "plt.title(\"Cost-Based Threshold Selection (Validation Set)\")\n",
    "\n",
    "# Annotate the two optimal thresholds found\n",
    "t_formula = res[\"threshold_formula\"]\n",
    "t_emp = res[\"threshold_empirical\"]\n",
    "plt.axvline(t_formula, linestyle=\"--\", color=\"red\", label=f\"Bayes Optimal Thr={t_formula:.2f}\")\n",
    "plt.axvline(t_emp, linestyle=\":\", color=\"black\", label=f\"Empirical Min Thr={t_emp:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81633a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the empirically optimal threshold from the previous analysis\n",
    "THR = res[\"threshold_empirical\"]\n",
    "\n",
    "# For visualization, we need a \"recall_floor\" to draw the horizontal line.\n",
    "# We'll use the actual recall achieved at our chosen cost-based threshold.\n",
    "y_pred_val = (y_score_val >= THR).astype(int)\n",
    "recall_at_thr = recall_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Visualizing performance at the empirical cost-based threshold of {THR:.3f}:\")\n",
    "plot_recall_floor_curves(y_val, y_score_val, recall_floor=recall_at_thr, chosen_threshold=THR)\n",
    "plot_cumulative_recall_at_threshold(y_val, y_score_val, chosen_threshold=THR)\n",
    "plot_topk_at_threshold(y_val, y_score_val, chosen_threshold=THR, top_k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e712ce",
   "metadata": {},
   "source": [
    "# 8. Final Threshold Selection and Test Set Evaluation\n",
    "\n",
    "This is a crucial step that often involves discussion with clinical stakeholders to align the model's operating point with clinical goals, patient safety requirements, and resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8947a6",
   "metadata": {},
   "source": [
    "| Thresholding Method | Objective | Pros (Best for...) | Cons (Potential Risks) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1. Workload Constrained** | Maximize TP (addiction risk) under a fixed (alert) budget | Good for limited resources, ensuring Ops stability | A low budget will miss many high-risk patients |\n",
    "| **2. Recall Floor** | Maximize precision (TP vs. FP) while holding a minimum recall rate (breath) | Acts as a safety net, ensuring a minimum capture rate | Increases false alarms e.g. workloads from alternative treatments, untreated pain |\n",
    "| **3. Cost-Based** | Minimize total expected cost by assigning costs to errors | Provides a formal f/ work to balance different clinical errors | Highly dependent on accurate cost estimates (can be subjective) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be2149",
   "metadata": {},
   "source": [
    "1. Compute the candidate threshold (THR) for each policy on the validation set\n",
    "2. Compare them on consistent operational metrics and expected cost\n",
    "3. Lock the final threshold using the same (THR) variable, to get an unbiased estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da659da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds are derived on validation, comparison is on test\n",
    "\n",
    "# Scores\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "\n",
    "# Derive thresholds on validation only, for the three methods\n",
    "res_rec  = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "thr_rec  = float(res_rec[\"threshold\"])\n",
    "\n",
    "res_work = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "thr_work = float(res_work[\"threshold\"])\n",
    "\n",
    "res_cost = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "thr_cost_emp = float(res_cost[\"threshold_empirical\"])\n",
    "\n",
    "# Compare policies on the unseen test set using the frozen (validation) thresholds\n",
    "rows = []\n",
    "for approach, thr in [\n",
    "    (\"1. Workload budget\", thr_work),\n",
    "    (\"2. Recall floor\",    thr_rec),\n",
    "    (\"3. Cost min emp\",    thr_cost_emp),\n",
    "]:\n",
    "    s = summary_at_threshold(y_test, y_score_test, thr).iloc[0].to_dict()\n",
    "    s.update({\n",
    "        \"approach\": approach,\n",
    "        \"expected_cost\": float(C_FP * s[\"FP\"] + C_FN * s[\"FN\"]),\n",
    "    })\n",
    "    rows.append(s)\n",
    "\n",
    "test_compare = (\n",
    "    pd.DataFrame(rows)[\n",
    "        [\"approach\", \"threshold\", \"precision\", \"recall\",\n",
    "         \"alerts_per_1000\", \"true_pos_per_1000\", \"TP\", \"FP\", \"FN\", \"expected_cost\"]\n",
    "    ]\n",
    "    .sort_values(\"approach\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "pivot_df = (\n",
    "    test_compare.set_index(\"approach\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    ")\n",
    "\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# Also report threshold-independent curve metrics on test for context\n",
    "test_roc_auc = float(roc_auc_score(y_test, y_score_test))\n",
    "test_pr_auc  = float(average_precision_score(y_test, y_score_test))\n",
    "print(f\"Test ROC AUC={test_roc_auc:.3f}  |  Test PR AUC={test_pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648aedc",
   "metadata": {},
   "source": [
    "Based on this scenario:\n",
    "\n",
    "- **Workload Constraint (THR = 0.38)**: Offers the highest precision (0.667) but the lowest recall (0.222), missing 7 of the 9 actual OD cases. This might be operationally efficient but carries a high patient risk i.e. low coverage for all patients\n",
    "\n",
    "- **Recall Floor (THR = 0.28)**: Guarantees a minimum level of patient safety by capturing over half the cases (recall = 0.556), at the cost of more false positive alerts i.e. more patients without opioid treatment (alternative treatments $$$)\n",
    "\n",
    "- **Cost-Based (THR = 0.17)**: Provides the highest recall (0.889) and lowest \"expected cost\" based on our 10:1 cost ratio. However, it generates the most alerts (460 per 1000 patients), which may not be operationally feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65540e10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_compare = test_compare.sort_values('approach').reset_index(drop=True)\n",
    "\n",
    "# Extract stats for each approach from the DataFrame\n",
    "workload_stats = test_compare.iloc[0]\n",
    "recall_stats = test_compare.iloc[1]\n",
    "cost_stats = test_compare.iloc[2]\n",
    "\n",
    "# Calculate total positive cases in the test set\n",
    "total_pos_test = workload_stats['TP'] + workload_stats['FN']\n",
    "\n",
    "# Dynamic markdown with increased font size using HTML <div>\n",
    "markdown_text = f\"\"\"\n",
    "* **Workload Constraint (THR = {workload_stats['threshold']:.2f}):** Offers the highest precision ({workload_stats['precision']:.3f}) but the lowest recall ({workload_stats['recall']:.3f}), missing {workload_stats['FN']:.0f} of the {total_pos_test:.0f} actual OD cases. This might be operationally efficient but carries a high patient risk\n",
    "* **Recall Floor (THR = {recall_stats['threshold']:.2f}):** Guarantees a minimum level of patient safety by capturing over half the cases (recall = {recall_stats['recall']:.3f}), at the cost of more false positive alerts\n",
    "* **Cost-Based (THR = {cost_stats['threshold']:.2f}):** Provides the highest recall ({cost_stats['recall']:.3f}) and lowest \"expected cost\" based on our 10:1 cost ratio. However, it generates the most alerts ({cost_stats['alerts_per_1000']:.0f} per 1000 patients), which may not be operationally feasible\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88cd2c",
   "metadata": {},
   "source": [
    "**Potential Decision:** For this clinical use case, patient safety is paramount. Missing a potential opioid use disorder case (a False Negative) has a significantly higher societal and health cost than a false alarm (a False Positive). Therefore, we will adopt the **2. Recall Floor threshold** of 0.28.\n",
    "\n",
    "This choice ensures we identify a majority of at-risk patients (recall > 0.5) while maintaining a manageable number of alerts for clinical review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99676342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scores and validation-derived thresholds (from your original code) ---\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "\n",
    "res_rec  = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "thr_rec  = float(res_rec[\"threshold\"])\n",
    "\n",
    "res_work = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "thr_work = float(res_work[\"threshold\"])\n",
    "\n",
    "res_cost = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "thr_cost_emp = float(res_cost[\"threshold_empirical\"])\n",
    "\n",
    "# --- Find the F1-optimal threshold on the VALIDATION set ---\n",
    "thresholds_f1 = np.linspace(0.01, 0.99, 100)\n",
    "f1_scores_val = [f1_score(y_val, (y_score_val >= thr).astype(int)) for thr in thresholds_f1]\n",
    "thr_f1_optimal = thresholds_f1[np.argmax(f1_scores_val)]\n",
    "\n",
    "\n",
    "# --- Compare all policies on the TEST set, now including F1 score ---\n",
    "rows = []\n",
    "# Add the new F1-optimized approach to the list\n",
    "approaches = [\n",
    "    (\"1. Workload budget\", thr_work),\n",
    "    (\"2. Recall floor\",    thr_rec),\n",
    "    (\"3. Cost min emp\",    thr_cost_emp),\n",
    "    (\"4. F1 Optimized\",    thr_f1_optimal)\n",
    "]\n",
    "\n",
    "for approach, thr in approaches:\n",
    "    # Get the standard summary metrics on the test set\n",
    "    s = summary_at_threshold(y_test, y_score_test, thr).iloc[0].to_dict()\n",
    "    \n",
    "    # Calculate the F1 score on the test set for this threshold\n",
    "    y_pred_test = (y_score_test >= thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Update the dictionary with all relevant info\n",
    "    s.update({\n",
    "        \"approach\": approach,\n",
    "        \"expected_cost\": float(C_FP * s[\"FP\"] + C_FN * s[\"FN\"]),\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "    rows.append(s)\n",
    "\n",
    "# Create the final comparison DataFrame, now including the \"F1 Score\" column\n",
    "test_compare = (\n",
    "    pd.DataFrame(rows)[\n",
    "        [\"approach\", \"threshold\", \"precision\", \"recall\", \"F1 Score\",\n",
    "         \"alerts_per_1000\", \"true_pos_per_1000\", \"TP\", \"FP\", \"FN\", \"expected_cost\"]\n",
    "    ]\n",
    "    .sort_values(\"approach\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Pivot the DataFrame for the final transposed view\n",
    "pivot_df = (\n",
    "    test_compare.set_index(\"approach\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    ")\n",
    "\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# --- Original threshold-independent metrics ---\n",
    "test_roc_auc = float(roc_auc_score(y_test, y_score_test))\n",
    "test_pr_auc  = float(average_precision_score(y_test, y_score_test))\n",
    "print(f\"Test ROC AUC={test_roc_auc:.3f}  |  Test PR AUC={test_pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed882ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Threshold Selection (from your \"Recall Floor\" policy) ---\n",
    "# This ensures THR is set to the value derived from the validation set analysis.\n",
    "THR = thr_rec\n",
    "\n",
    "print(f\"--- Visualizing Final Performance on Test Set at Threshold = {THR:.2f} ---\")\n",
    "\n",
    "# --- Step 1: Get scores and metrics on the TEST set ---\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "\n",
    "# For the first plot, we need the actual recall achieved on the test set by our threshold.\n",
    "y_pred_test = (y_score_test >= THR).astype(int)\n",
    "recall_at_thr_test = recall_score(y_test, y_pred_test)\n",
    "\n",
    "\n",
    "# --- Step 2: Generate the plots sequentially using your utils functions ---\n",
    "\n",
    "# Plot 1: Recall Floor Curves\n",
    "print(\"\\\\n1. Precision-Recall Curve with Final Threshold and Resulting Recall\")\n",
    "plot_recall_floor_curves(y_true=y_test,\n",
    "                         y_score=y_score_test,\n",
    "                         recall_floor=recall_at_thr_test,\n",
    "                         chosen_threshold=THR)\n",
    "\n",
    "# Plot 2: Cumulative Recall vs. Alerts\n",
    "print(\"\\\\n2. Cumulative Recall vs. Number of Alerts\")\n",
    "plot_cumulative_recall_at_threshold(y_true=y_test,\n",
    "                                     y_score=y_score_test,\n",
    "                                     chosen_threshold=THR)\n",
    "\n",
    "# Plot 3: Top-k Patient Prioritization\n",
    "print(\"\\\\n3. Top 30 Highest-Risk Patients View\")\n",
    "plot_topk_at_threshold(y_true=y_test,\n",
    "                       y_score=y_score_test,\n",
    "                       chosen_threshold=THR,\n",
    "                       top_k=30)\n",
    "\n",
    "# Generate the report and plots for the final model on the test set\n",
    "auc_report(y_true=y_test,\n",
    "           y_score=y_score_test,\n",
    "           name=\"Final Calibrated Model on Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581b38e",
   "metadata": {},
   "source": [
    "# 9. Intro to RAI toolkit\n",
    "While global metrics e.g. ROC AUC and PR AUC, are essential for evaluating model performance, they don't tell the full story. To **responsibly deploy a model in a clinical setting, we must look deeper**. We need to understand:\n",
    "\n",
    "- **Why** it makes certain predictions (Interpretability)\n",
    "- **Where** it fails and for whom (Error Analysis)\n",
    "- **How** we might change an outcome (Counterfactuals)\n",
    "- **What** the real-world impact of an intervention might be (Causal Inference)\n",
    "\n",
    "Tools such as MS's RAI toolkit provides a unified dashboard to explore these questions. In this section, we will build the dashboard step-by-step for our calibrated opioid risk model:\n",
    "\n",
    "1. Setup: Prepare the data and model and initialize the main RAIInsights object.\n",
    "2. Add Components: Add the advanced analytical tools like the explainer and error analysis.\n",
    "3. Compute & Launch: Run the analyses and launch the interactive dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de40c4",
   "metadata": {},
   "source": [
    "## 9.1 Setup: Creating the RAIInsights Object\n",
    "\n",
    "1. Combine Data: The toolkit requires the training and testing data to be in pandas DataFrames (including features and target)\n",
    "2. Define Metadata: We must tell the toolkit which features are categorical and which represents a \"sensitive\" group for fairness analysis e.g. Low_inc as \"Identity group\"\n",
    "3. Wrap the Model: We wrap our calibrated classifier in a helper class (in utils) to apply the threshold during prediction\n",
    "\n",
    "This set up creates a RAIInsights object, a container for all the analytical components we'll add next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39910e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the exact feature set the model was trained on\n",
    "feature_cols = list(X_train.columns)\n",
    "\n",
    "# 2. Create RAI-compatible DataFrames with features and the target column\n",
    "target_col = TARGET\n",
    "train_df_rai = X_train.copy()\n",
    "train_df_rai[target_col] = y_train\n",
    "test_df_rai = X_test.copy()\n",
    "test_df_rai[target_col] = y_test\n",
    "\n",
    "if \"Surgery\" in train_df_rai.columns:\n",
    "    train_df_rai[\"Surgery\"] = train_df_rai[\"Surgery\"].astype(str)\n",
    "    test_df_rai[\"Surgery\"] = test_df_rai[\"Surgery\"].astype(str)\n",
    "\n",
    "# 3. Define feature metadata\n",
    "categorical_features = [c for c in cat_like_binary_cols if c in feature_cols]\n",
    "if \"Surgery\" in feature_cols and \"Surgery\" not in categorical_features:\n",
    "    categorical_features.append(\"Surgery\")\n",
    "\n",
    "identity_feature = \"Low_inc\" if \"Low_inc\" in feature_cols else None\n",
    "feature_metadata = FeatureMetadata(\n",
    "    identity_feature_name=identity_feature,\n",
    "    categorical_features=categorical_features,\n",
    ")\n",
    "\n",
    "# 4. Lock the final model threshold from the recall-floor policy\n",
    "FINAL_THR = float(thr_rec)\n",
    "print(f\"Final threshold selected from the recall-floor policy: {FINAL_THR:.3f}\")\n",
    "\n",
    "# 5. Wrap the classifier to make predictions based on our final threshold\n",
    "policy_model = make_thresholded_estimator(calibrated_clf, threshold=FINAL_THR)\n",
    "\n",
    "# 6. Create the main RAIInsights object\n",
    "rai_insights = RAIInsights(\n",
    "    model=policy_model,\n",
    "    train=train_df_rai,\n",
    "    test=test_df_rai,\n",
    "    target_column=target_col,\n",
    "    task_type=\"classification\",\n",
    "    feature_metadata=feature_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d3222",
   "metadata": {},
   "source": [
    "## 9.2 Adding Analytical Components to the Dashboard\n",
    "\n",
    "We add the core analytical components to our RAIInsights object\"\n",
    "\n",
    "- Interpretability (.explainer): To understand the why behind model predictions\n",
    "- Error Analysis (.error_analysis): To diagnose where the model is failing\n",
    "- Counterfactuals (.counterfactual): To generate \"what-if\" scenarios for individual patients\n",
    "- Causal Inference (.causal): To estimate the real-world impact of interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the core analytical components.\n",
    "# Data Balance and Model Analysis views are generated automatically.\n",
    "rai_insights.explainer.add()\n",
    "rai_insights.error_analysis.add()\n",
    "\n",
    "# Use the exact observed categories from TRAIN for Surgery\n",
    "surgery_categories = (\n",
    "    train_df_rai[\"Surgery\"].astype(str).dropna().unique().tolist()\n",
    "    if \"Surgery\" in train_df_rai.columns else None\n",
    ")\n",
    "\n",
    "# Define features for counterfactual and causal analysis (12, 34, 31)\n",
    "permitted_range = {\n",
    "    \"rx_ds\": [float(X_train[\"rx_ds\"].min()), float(X_train[\"rx_ds\"].max())],\n",
    "    \"Surgery\": [\"0\", \"1\"]\n",
    "}\n",
    "\n",
    "if surgery_categories is not None:\n",
    "    permitted_range[\"Surgery\"] = sorted(surgery_categories)  # e.g. ['0', '1']\n",
    "\n",
    "features_to_vary = [\n",
    "    \"rx_ds\", \"Surgery\"] if \"Surgery\" in feature_cols else [\"rx_ds\"]\n",
    "\n",
    "# Add the remaining components\n",
    "rai_insights.counterfactual.add(\n",
    "    total_CFs=10,\n",
    "    method=\"random\",  # or genetic / kdtree\n",
    "    desired_class=\"opposite\",  # or \"same\"\n",
    "    features_to_vary=features_to_vary,\n",
    "    permitted_range=permitted_range\n",
    ")\n",
    "\n",
    "# Define treatment features for causal analysis\n",
    "# Can a healthcare provider directly and ethically change this factor to observe a potential change in the patient's OD risk?\n",
    "# e.g. we can't change \"Low_inc\" (socioeconomic status) or \"V\" (traumatic injury) directly\n",
    "# <- \"What if we successfully managed this patient's underlying condition?\"\n",
    "treatment_features = [\"Surgery\", \"rx_ds\"]\n",
    "rai_insights.causal.add(treatment_features=treatment_features)\n",
    "\n",
    "print(\"Explainer, Error Analysis, Counterfactual, and Causal components added \u2705\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac380c59",
   "metadata": {},
   "source": [
    "## 9.3 Compute Insights and Launch Dashboard\n",
    "Now we run the compute() method, which executes all the analyses in the background. Launch the interactive dashboard to explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab81221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all the added insights. This may take a few minutes.\n",
    "print(\"Computing RAI insights... Please wait.\")\n",
    "rai_insights.compute()\n",
    "print(\"Computation complete.\")\n",
    "# Launch the interactive dashboard\n",
    "ResponsibleAIDashboard(rai_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b0d5c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de0071fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "851cc04e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6736f1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628bb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_rai_mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}