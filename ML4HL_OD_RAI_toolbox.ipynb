{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b54679",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is part of a machine learning for healthcare exercise, focusing on using the Responsible AI (RAI) package to enhance clinical decision-making. The toolkit will be used to analyze opioid use disorder (OD) risk, with three key objectives:\n",
    "\n",
    "1. Analyze Errors and Explore Interpretability of Models: We will run Interpret-Community’s global explainers to generate feature importance insights and visualize model errors with the Error Analysis dashboard\n",
    "\n",
    "2. Plan real-world action through counterfactual and causal analysis: By leveraging counterfactual examples and causal inference, we will explore decision-making strategies based on opioid prescription patterns and patient comorbidities to understand possible interventions and their impacts\n",
    "\n",
    "3. Assess addiction risk predictions: A classification model trained on patient-level features (income, surgeries, opioid prescription days, and comorbidities A–V) will be evaluated to examine its performance in predicting risk of opioid use disorder and to inform prevention strategies\n",
    "\n",
    "**The goal is to provide non-trivial insights for clinical decision making, leveraging machine learning paired with responsible AI tools, to improve patient outcomes in the healthcare context.**\n",
    "\n",
    "Based on notebooks from the [Responsible AI toolkit](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934c151",
   "metadata": {},
   "source": [
    "# Data Set Characteristics\n",
    "\n",
    "Number of Instances: patient-level records (rows)\n",
    "\n",
    "Number of Attributes: 20 predictive attributes and 1 target class\n",
    "\n",
    "Attribute Information:\n",
    "- OD (target): whether the patient had an opioid use disorder diagnosis (binary: 1 = yes, 0 = no)\n",
    "- Low_inc: low income flag (1 = low income, 0 = not low income)\n",
    "- Surgery: whether the patient underwent major surgery in the 2 years\n",
    "- rx_ds: number of days of prescribed opioids in the 2 years\n",
    "- A: infectious diseases group A (binary flag)\n",
    "- B: infectious diseases group B\n",
    "- C: malignant neoplasm\n",
    "- D: benign neoplasm\n",
    "- E: endocrine conditions\n",
    "- F: mental and behavioral health conditions (excluding opioid-related)\n",
    "- H: ear conditions\n",
    "- I: circulatory system conditions\n",
    "- J: respiratory system conditions\n",
    "- K: digestive system conditions\n",
    "- L: skin conditions\n",
    "- M: musculoskeletal system conditions\n",
    "- N: urinary system conditions\n",
    "- R: other signs and symptoms\n",
    "- S: injuries\n",
    "- T: burns and toxic conditions\n",
    "- V: external trauma conditions\n",
    "\n",
    "class:\n",
    "- OD = 1: patient identified with opioid use disorder in the 2 years\n",
    "- OD = 0: patient without opioid use disorder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da36d3",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "- responsibleai and raiwidgets provide RAIInsights and the dashboard\n",
    "- fairlearn provides fairness metrics and mitigation algorithms used under the hood\n",
    "- imbalanced-learn offers resampling utilities if you want to experiment with imbalance mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f488354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, false_positive_rate\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from raiwidgets import ResponsibleAIDashboard\n",
    "from responsibleai import RAIInsights\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.base import clone\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,        # Area under ROC, measures ranking ability across thresholds\n",
    "    average_precision_score,  # Area under precision-recall curve, better under imbalance\n",
    "    # Mean squared error of predicted probabilities, lower = better calibration\n",
    "    brier_score_loss,\n",
    "    log_loss,             # Cross-entropy penalty, punishes overconfident wrong predictions\n",
    "    matthews_corrcoef,    # Balanced correlation metric, robust when classes are imbalanced\n",
    "    confusion_matrix,     # Counts of TP, FP, TN, FN at a given threshold\n",
    "    precision_recall_curve,  # Gives precision and recall values at different thresholds\n",
    "    auc,                  # Generic trapezoidal area under a curve\n",
    "    precision_score,      # Share of positive predictions that are correct\n",
    "    recall_score,         # Share of true positives the model actually finds\n",
    "    RocCurveDisplay,      # Quick ROC curve plotting helper\n",
    "    PrecisionRecallDisplay  # Quick precision-recall curve plotting helper\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import cast\n",
    "sys.path.append(\"/mnt/data\")\n",
    "from utils import (\n",
    "    positive_scores,\n",
    "    auc_report,\n",
    "    tradeoff_table,\n",
    "    pick_threshold_cost,\n",
    "    pick_threshold_recall_floor,\n",
    "    pick_threshold_workload,\n",
    "    summary_at_threshold,\n",
    "    plot_recall_floor_curves,\n",
    "    plot_cumulative_recall_at_threshold,\n",
    "    plot_topk_at_threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Fairness utilities for quick, programmatic checks\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc05cf",
   "metadata": {},
   "source": [
    "# 2. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f881f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 22)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "OD",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Low_inc",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SURG",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rx ds",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "A",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "B",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "C",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "E",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "F",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "H",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "I",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "J",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "K",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "L",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "M",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "N",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "R",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "T",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "V",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9a73147f-9e59-4e51-a527-2d3b895d9989",
       "rows": [
        [
         "0",
         "1",
         "1",
         "1",
         "0",
         "516",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "1",
         "1",
         "0",
         "1",
         "1",
         "1",
         "0",
         "1",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "2",
         "1",
         "1",
         "0",
         "119",
         "0",
         "0",
         "1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0"
        ],
        [
         "2",
         "3",
         "1",
         "1",
         "1",
         "925",
         "1",
         "1",
         "0",
         "1",
         "1",
         "0",
         "1",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0"
        ],
        [
         "3",
         "4",
         "0",
         "1",
         "0",
         "393",
         "1",
         "1",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "0",
         "1",
         "1",
         "1",
         "0"
        ],
        [
         "4",
         "5",
         "1",
         "1",
         "0",
         "630",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 22,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>OD</th>\n",
       "      <th>Low_inc</th>\n",
       "      <th>SURG</th>\n",
       "      <th>rx ds</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>...</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>925</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>393</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  OD  Low_inc  SURG  rx ds  A  B  C  D  E  ...  I  J  K  L  M  N  R  S  \\\n",
       "0   1   1        1     0    516  0  0  0  1  0  ...  1  0  1  1  1  0  1  0   \n",
       "1   2   1        1     0    119  0  0  1  0  0  ...  1  1  0  0  1  1  1  1   \n",
       "2   3   1        1     1    925  1  1  0  1  1  ...  1  0  0  0  1  0  1  0   \n",
       "3   4   0        1     0    393  1  1  0  0  0  ...  0  0  1  1  1  0  1  1   \n",
       "4   5   1        1     0    630  0  0  0  0  1  ...  0  0  0  0  1  1  1  0   \n",
       "\n",
       "   T  V  \n",
       "0  0  0  \n",
       "1  0  0  \n",
       "2  1  0  \n",
       "3  1  0  \n",
       "4  0  0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to your CSV (update if needed for your system)\n",
    "DATA_PATH = \"./Data/opiod_raw_data.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "866f3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome counts:\n",
      "0    819\n",
      "1    181\n",
      "Name: OD, dtype: int64\n",
      "\n",
      "Prevalence: 18.10%\n",
      "Patients with OD per 1000: 181.0\n"
     ]
    }
   ],
   "source": [
    "# Outcome prevalence and missingness\n",
    "\n",
    "TARGET = \"OD\"  # target: 1 = opioid use disorder diagnosis, 0 = none\n",
    "\n",
    "# Outcome prevalence\n",
    "counts = df_raw[TARGET].value_counts(dropna=False)\n",
    "prevalence_percent = counts[1] / counts.sum() * 100\n",
    "positives_per_1000 = counts[1] / counts.sum() * 1000\n",
    "\n",
    "print(\"Outcome counts:\")\n",
    "print(counts)\n",
    "print(f\"\\nPrevalence: {prevalence_percent:.2f}%\")\n",
    "print(f\"Patients with OD per 1000: {positives_per_1000:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142570c9",
   "metadata": {},
   "source": [
    "# 3. Basic cleaning and schema alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9316e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the raw DataFrame\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [c.strip().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "              for c in df.columns]\n",
    "\n",
    "# Drop ID column\n",
    "if df.shape[1] > 0:\n",
    "    df = df.drop(columns=[df.columns[0]])\n",
    "\n",
    "# Harmonize known aliases\n",
    "if \"SURG\" in df.columns and \"Surgery\" not in df.columns:\n",
    "    df = df.rename(columns={\"SURG\": \"Surgery\"})\n",
    "\n",
    "# Expected columns from the data dictionary\n",
    "expected_cols = [\n",
    "    \"OD\", \"Low_inc\", \"Surgery\", \"rx_ds\",\n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"R\", \"S\", \"T\", \"V\"\n",
    "]\n",
    "\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# Helper to coerce various binary encodings to 0/1\n",
    "\n",
    "\n",
    "def to_binary(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype == \"O\":\n",
    "        mapped = s.astype(str).str.strip().str.lower().map({\n",
    "            \"1\": 1, \"0\": 0,\n",
    "            \"y\": 1, \"n\": 0,\n",
    "            \"yes\": 1, \"no\": 0,\n",
    "            \"true\": 1, \"false\": 0\n",
    "        })\n",
    "        s = pd.to_numeric(mapped, errors=\"coerce\")\n",
    "    else:\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return (s.fillna(0) > 0).astype(int)\n",
    "\n",
    "\n",
    "# Target is binary 0/1\n",
    "df[\"OD\"] = to_binary(df[\"OD\"])\n",
    "\n",
    "# rx_ds is numeric count of opioid prescription days\n",
    "df[\"rx_ds\"] = pd.to_numeric(df[\"rx_ds\"], errors=\"coerce\")\n",
    "\n",
    "# Binary predictors: Low_inc, Surgery, and A..V\n",
    "binary_cols = [\"Low_inc\", \"Surgery\", \"A\", \"B\", \"C\", \"D\", \"E\",\n",
    "               \"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"R\", \"S\", \"T\", \"V\"]\n",
    "df[binary_cols] = df[binary_cols].apply(to_binary)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34345072",
   "metadata": {},
   "source": [
    "# 4. Feature matrix and 80/15/5 stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"OD\"\n",
    "num_cols = [\"rx_ds\"]\n",
    "cat_like_binary_cols = binary_cols.copy()\n",
    "\n",
    "X = df[num_cols + cat_like_binary_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Split temp into validation and test, 15% each overall\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val:\",   X_val.shape,   \"y_val:\",   y_val.shape)\n",
    "print(\"X_test:\",  X_test.shape,  \"y_test:\",  y_test.shape)\n",
    "\n",
    "# Sanity check class balance\n",
    "\n",
    "\n",
    "def _dist(s):\n",
    "    vc = s.value_counts(normalize=True).sort_index()\n",
    "    return {\"p(OD=0)\": float(vc.get(0, 0.0)), \"p(OD=1)\": float(vc.get(1, 0.0))}\n",
    "\n",
    "\n",
    "print(\"Class distribution overall:\", _dist(y))\n",
    "print(\"Class distribution train:\",   _dist(y_train))\n",
    "print(\"Class distribution val:\",     _dist(y_val))\n",
    "print(\"Class distribution test:\",    _dist(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0a7ff",
   "metadata": {},
   "source": [
    "# 5. Modeling pipeline, training, and calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63d7b7",
   "metadata": {},
   "source": [
    "## Setting a baseline\n",
    "A naive majority-class baseline clarifies the minimum standard any model must beat, highlighting the danger of ignoring minority patients and ensuring improvements carry meaningful weight in healthcare decision making\n",
    "\n",
    "**ROC AUC**  \n",
    "- 0.5 → no discrimination\n",
    "- 0.6–0.7 → poor\n",
    "- 0.7–0.8 → fair\n",
    "- 0.8–0.9 → good\n",
    "- ≥ 0.9 → excellent\n",
    "\n",
    "**PR AUC**  \n",
    "- Must be interpreted against event prevalence `p` in the validation set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: DummyClassifier (majority class)\n",
    "dummy_clf = DummyClassifier(\n",
    "    strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "proba_val = dummy_clf.predict_proba(X_val)\n",
    "pos_idx = int(np.where(dummy_clf.classes_ == 1)[0][0])  # index for class \"1\"\n",
    "y_score_val = np.asarray(proba_val)[:, pos_idx]\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(dummy_clf, X_val)\n",
    "metrics_dummy = auc_report(\n",
    "    y_val, y_score_val, name=\"Dummy baseline\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bc2a2",
   "metadata": {},
   "source": [
    "## Preprocesing our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74d963",
   "metadata": {},
   "source": [
    "[Scikit-learn preprocessing](https://scikit-learn.org/stable/api/sklearn.preprocessing.html) standardizes and transforms features for modeling, including scaling, encoding, and imputation. Helping to maintain a consistent data transformation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True))\n",
    "])\n",
    "\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"bin\", binary_transformer, cat_like_binary_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",  # drops any column not in previously specified\n",
    "    verbose_feature_names_out=False  # keeps original feature names\n",
    ")\n",
    "\n",
    "# Logistic Regression baseline with variance filter\n",
    "base_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    # Eliminates features with zero variance\n",
    "    (\"varth\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"liblinear\",  # See details in course material\n",
    "        class_weight=\"balanced\",  # Adjusts weights for class imbalance\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=200\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8ccc7",
   "metadata": {},
   "source": [
    "## Basic preliminary model performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit base model\n",
    "base_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(base_clf, X_val)\n",
    "metrics_base = auc_report(y_val, y_score_val, name=\"base_clf\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e413ac1",
   "metadata": {},
   "source": [
    "## Recalibrating the probabilities\n",
    "\n",
    "**1. Reliable probabilities**  \n",
    "- Turns raw scores into real probabilities  \n",
    "- Ensures predictions match observed outcome frequencies  \n",
    "- Prevents overly high or low risk estimates  \n",
    "\n",
    "**2. Better clinical decisions**  \n",
    "- Essential when risk values guide medical choices  \n",
    "- Supports thresholds with clear clinical meaning\n",
    "- Reduces wasted clinical resources  \n",
    "\n",
    "**3. Trust and adoption**  \n",
    "- Builds trust in AI decisions  \n",
    "- Enables safer patient outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate probabilities using CV on training data\n",
    "calibrated_clf = CalibratedClassifierCV(\n",
    "    estimator=base_clf,\n",
    "    method=\"sigmoid\",\n",
    "    cv=5\n",
    ")\n",
    "calibrated_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile bands on test only\n",
    "base_clf_uncal = clone(base_clf).fit(X_train, y_train)\n",
    "p_before = base_clf_uncal.predict_proba(X_test)[:, 1]\n",
    "p_after = calibrated_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_cal = pd.DataFrame({\"Actual_OD\": y_test.values, \"Pred_before\": p_before,\n",
    "                      \"Pred_after\": p_after}, index=X_test.index)\n",
    "\n",
    "# 10 quantile bands with similar counts\n",
    "df_cal[\"Risk_band\"] = pd.qcut(df_cal[\"Pred_after\"], q=10, labels=[\n",
    "                              f\"Q{i}\" for i in range(1, 11)], duplicates=\"drop\")\n",
    "\n",
    "summary = (\n",
    "    df_cal.groupby(\"Risk_band\", observed=True)\n",
    "    .agg(Patients=(\"Actual_OD\", \"size\"),\n",
    "         Avg_pred_before=(\"Pred_before\", \"mean\"),\n",
    "         Actual_OD_rate=(\"Actual_OD\", \"mean\"),\n",
    "         Avg_pred_after=(\"Pred_after\", \"mean\"))\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "print(\"Check sizes\")\n",
    "print(\"len(X_train) =\", len(X_train), \"len(X_val) =\",\n",
    "      len(X_val), \"len(X_test) =\", len(X_test))\n",
    "print(\"Rows in table sum to\", int(summary[\"Patients\"].sum()))\n",
    "\n",
    "# Reliability plot using the same fixed risk bands summary\n",
    "print(\"Points near the diagonal mean predicted risk matches observed OD frequency\")\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\",\n",
    "         linewidth=1, label=\"Perfectly calibrated\")\n",
    "plt.scatter(summary[\"Avg_pred_before\"],\n",
    "            summary[\"Actual_OD_rate\"], label=\"Before calibration\")\n",
    "plt.scatter(summary[\"Avg_pred_after\"],\n",
    "            summary[\"Actual_OD_rate\"],  label=\"After calibration\")\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Observed OD rate\")\n",
    "plt.title(\"Calibration reliability by risk bands\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234181a1",
   "metadata": {},
   "source": [
    "## Compare base_clf vs calibrated_clf on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d490f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on discrimination equality and calibration improvements\n",
    "\n",
    "# Scores\n",
    "y_score_val_base = positive_scores(base_clf, X_val)\n",
    "y_score_val_cal = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Discrimination\n",
    "roc_base = roc_auc_score(y_val, y_score_val_base)\n",
    "pr_base = average_precision_score(y_val, y_score_val_base)\n",
    "roc_cal = roc_auc_score(y_val, y_score_val_cal)\n",
    "pr_cal = average_precision_score(y_val, y_score_val_cal)\n",
    "\n",
    "# Ranking correlation\n",
    "rho_s, _ = spearmanr(y_score_val_base, y_score_val_cal)\n",
    "tau_k, _ = kendalltau(y_score_val_base, y_score_val_cal)\n",
    "\n",
    "# Calibration\n",
    "ll_base = log_loss(y_val, np.clip(y_score_val_base, 1e-6, 1 - 1e-6))\n",
    "ll_cal = log_loss(y_val, np.clip(y_score_val_cal,  1e-6, 1 - 1e-6))\n",
    "brier_base = brier_score_loss(y_val, y_score_val_base)\n",
    "brier_cal = brier_score_loss(y_val, y_score_val_cal)\n",
    "\n",
    "# Assemble into dataframe\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"Metric\": \"ROC AUC\", \"Base\": roc_base, \"Calibrated\": roc_cal},\n",
    "    {\"Metric\": \"PR AUC\", \"Base\": pr_base, \"Calibrated\": pr_cal},\n",
    "    {\"Metric\": \"Spearman rank corr\", \"Base\": rho_s, \"Calibrated\": rho_s},\n",
    "    {\"Metric\": \"Kendall tau\", \"Base\": tau_k, \"Calibrated\": tau_k},\n",
    "    # Penalises wrong over-confident predictions\n",
    "    {\"Metric\": \"Log loss\", \"Base\": ll_base, \"Calibrated\": ll_cal},\n",
    "    {\"Metric\": \"Brier score\", \"Base\": brier_base,\n",
    "        \"Calibrated\": brier_cal},  # MSE of predicted probabilities\n",
    "])\n",
    "\n",
    "display(metrics_df.style.format({\"Base\": \"{:.3f}\", \"Calibrated\": \"{:.3f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c73af",
   "metadata": {},
   "source": [
    "## Deciding where to cut off i.e. what probability is “high risk enough” to trigger an intervention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96638f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get calibrated probabilities\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Define a grid of thresholds\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "rows = []\n",
    "for thr in thresholds:\n",
    "    y_pred = (y_score_val >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    alerts_per_1000 = 1000 * np.mean(y_pred)\n",
    "    true_pos_per_1000 = 1000 * tp / len(y_val)\n",
    "    rows.append({\n",
    "        \"threshold\": thr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"alerts_per_1000\": alerts_per_1000,\n",
    "        \"true_pos_per_1000\": true_pos_per_1000,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "# Display a few candidate thresholds for discussion\n",
    "display(\n",
    "    df_thr.query(\"threshold in [0.1, 0.2, 0.3, 0.4, 0.5]\")\n",
    "         .round(3)\n",
    "         .set_index(\"threshold\")\n",
    ")\n",
    "\n",
    "# Plot workload vs threshold\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000 patients\")\n",
    "plt.title(\"Operational tradeoffs vs threshold (validation set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cd2f5",
   "metadata": {},
   "source": [
    "## Choosing an operating threshold\n",
    "\n",
    "Models produce probabilities, but clinicians are the ones taking decisions, and carring the accountability of their actions.\n",
    "\n",
    "- Setting a threshold balances in this case, among others, between missed addiction cases and unnecessary undertreatment of pain\n",
    "- Clear rules make these tradeoffs explicit, explainable, and auditable!\n",
    "\n",
    "We will run three threshold tuning analyses:\n",
    "1. **Workload constrained threshold**  \n",
    "  Capture the most true cases without exceeding a fixed alert capacity for the clinic\n",
    "2. **Recall floor then maximize precision**  \n",
    "  Guarantee a minimum case capture for safety, then pick the threshold with the fewest false alarms\n",
    "3. **Cost based threshold (Bayes rule)**  \n",
    "  Minimize expected harm using estimated costs for false negatives and false positives\n",
    "\n",
    "Readouts to watch\n",
    "- Threshold, precision, recall, alerts per 1000 patients, true positives per 1000, false positives, false negatives\n",
    "- Connect the chosen rule to clinical policy and resource capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f08220",
   "metadata": {},
   "source": [
    "### 1. Workload constrained threshold for calibrated_clf on validation\n",
    "\n",
    "- Alerts budget: maximum alerts per 1000 patients the clinic can review without overloading resources\n",
    "- Objective: within the alerts budget, choose the threshold that yields the most true positives per 1000 so more patients at real risk are correctly flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALERTS_BUDGET = 100.0  # alerts per 1000 patients\n",
    "\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "tbl = res[\"table\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.axhline(ALERTS_BUDGET, linestyle=\"--\")\n",
    "plt.axvline(res[\"threshold\"], linestyle=\":\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000\")\n",
    "plt.title(\"Workload constrained threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "display(summary_df.round(3).set_index(\"rule\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BINS = 10  # deciles by default\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Build bands by quantiles, highest risk = band 1\n",
    "bands = pd.qcut(y_score_val, q=N_BINS, labels=False, duplicates=\"drop\")\n",
    "# qcut labels lowest=0..highest=K-1, invert so 1 is highest-risk band\n",
    "bands = (bands.max() - bands) + 1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"y_true\": y_val.astype(int),\n",
    "    \"score\": y_score_val,\n",
    "    \"band\": bands.astype(int),\n",
    "})\n",
    "\n",
    "summ = (df.groupby(\"band\", as_index=True)\n",
    "          .agg(n=(\"y_true\",\"size\"),\n",
    "               positives=(\"y_true\",\"sum\"),\n",
    "               min_score=(\"score\",\"min\"),\n",
    "               max_score=(\"score\",\"max\"))\n",
    "          .sort_index())\n",
    "\n",
    "summ[\"prevalence\"] = summ[\"positives\"] / summ[\"n\"]\n",
    "summ[\"cum_capture\"] = summ[\"positives\"].cumsum() / df[\"y_true\"].sum()\n",
    "summ[\"alerts_per_1000\"] = 1000.0 * summ[\"n\"] / len(df)\n",
    "summ[\"true_pos_per_1000\"] = 1000.0 * summ[\"positives\"] / len(df)\n",
    "\n",
    "display(summ.round(3))\n",
    "\n",
    "# Optional threshold overlay if you already chose one, else set THR=None\n",
    "THR = None  # e.g., THR = 0.23\n",
    "\n",
    "# Histogram of risk scores with decile edges\n",
    "plt.figure()\n",
    "plt.hist(df[\"score\"], bins=30)\n",
    "if THR is not None:\n",
    "    plt.axvline(THR, linestyle=\"--\")\n",
    "# draw decile boundaries\n",
    "edges = np.quantile(df[\"score\"], np.linspace(0,1,N_BINS+1))\n",
    "for x in edges:\n",
    "    plt.axvline(x, linestyle=\":\", linewidth=0.8)\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Patients\")\n",
    "plt.title(\"Risk distribution with decile boundaries\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "THR = res[\"threshold\"] # <- use the threshold from the previous step\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "ids = np.arange(len(y_val))\n",
    "\n",
    "# Sort patients by predicted risk\n",
    "order = np.argsort(-y_score_val)\n",
    "top_idx = order[:30]   # top 30 for visualization\n",
    "top_scores = y_score_val[top_idx]\n",
    "top_true = np.asarray(y_val)[top_idx].astype(int)\n",
    "\n",
    "# Split indices for TP vs FP\n",
    "tp_idx = np.where(top_true == 1)[0]\n",
    "fp_idx = np.where(top_true == 0)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(tp_idx, top_scores[tp_idx], color=\"tab:red\", label=\"True addicted (TP)\")\n",
    "plt.bar(fp_idx, top_scores[fp_idx], color=\"tab:gray\", label=\"Not addicted (FP)\")\n",
    "plt.axhline(THR, linestyle=\"--\", color=\"black\", label=f\"Threshold = {THR:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Patients ranked by predicted risk\")\n",
    "plt.ylabel(\"Predicted risk\")\n",
    "plt.title(\"Top 50 highest-risk patients on validation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195459c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "order = np.argsort(-y_score_val)\n",
    "y_sorted = np.asarray(y_val)[order].astype(int)\n",
    "\n",
    "# Cumulative recall\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "total_pos = cum_tp[-1] if cum_tp.size else 0\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "recall_curve = cum_tp / total_pos if total_pos > 0 else np.zeros_like(cum_tp)\n",
    "\n",
    "# Budget for alerts\n",
    "n_budget = int(np.ceil(ALERTS_BUDGET * len(y_val) / 1000.0))\n",
    "\n",
    "# Recall at budget\n",
    "recall_at_budget = recall_curve[n_budget - 1] if n_budget > 0 and n_budget <= len(y_val) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, recall_curve, label=\"Cumulative recall\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", color=\"red\", label=f\"Budget = {n_budget} alerts\")\n",
    "\n",
    "# Annotate recall at budget\n",
    "plt.scatter(n_budget, recall_at_budget, color=\"black\", zorder=5)\n",
    "plt.text(n_budget + 2, recall_at_budget, f\"Recall = {recall_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Cumulative capture of true cases vs alerts\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative precision at top-k alerts\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "precision_curve = cum_tp / alerts\n",
    "\n",
    "# Alerts budget scaled to validation size\n",
    "prec_at_budget = precision_curve[n_budget - 1] if 0 < n_budget <= len(y_sorted) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, precision_curve, label=\"Precision at top-k alerts\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", label=f\"Budget = {n_budget} alerts\")\n",
    "plt.scatter(n_budget, prec_at_budget, zorder=5)\n",
    "plt.text(n_budget + max(2, len(y_sorted)//100), prec_at_budget, f\"Precision = {prec_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs number of alerts\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c8fbb",
   "metadata": {},
   "source": [
    "### 2. Recall floor then maximize precision for calibrated_clf on validation\n",
    "\n",
    "- Recall floor: minimum acceptable recall set by safety policy to limit missed addiction cases\n",
    "- Precision objective: among thresholds meeting the recall floor, pick the one with highest precision to reduce unnecessary undertreatment and clinician workload\n",
    "\n",
    "Deciding which recall floor to sue:\n",
    "1. The chosen floor is a value judgment balancing patient safety vs resource burden\n",
    "2. In medicine, it's often the case that missing a true case (false negative) is often much worse than raising extra alarms (false positives)\n",
    "3. A recall floor enforces a safety guarantee: the model must capture at least e.g. 60% of patients who will become addicted\n",
    "\n",
    "Among thresholds that satisfy recall ≥ 0.6, you then pick the one with the best precision, to minimize unnecessary undertreatment and workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af300cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: choose threshold by recall floor\n",
    "RECALL_FLOOR = 0.60 # <- judgment call\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "THR = float(res[\"threshold\"])\n",
    "\n",
    "# Step 2: numeric summary at the chosen threshold\n",
    "summary_df = summary_at_threshold(y_val, y_score_val, THR)\n",
    "display(summary_df.round(3).set_index(\"threshold\"))\n",
    "\n",
    "# Step 3: precision and recall vs threshold with annotations\n",
    "plot_recall_floor_curves(y_val, y_score_val, recall_floor=RECALL_FLOOR, chosen_threshold=THR)\n",
    "\n",
    "# Step 4: cumulative recall vs alerts with vertical line at alerts implied by THR\n",
    "plot_cumulative_recall_at_threshold(y_val, y_score_val, chosen_threshold=THR)\n",
    "\n",
    "# Step 5: patient-level prioritization view at THR\n",
    "plot_topk_at_threshold(y_val, y_score_val, chosen_threshold=THR, top_k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48885c",
   "metadata": {},
   "source": [
    "### 3. Cost based threshold for calibrated_clf on validation\n",
    "\n",
    "- C_FN missed addiction case: downstream care for opioid use disorder, overdose treatment, long-term morbidity, lost productivity, family and social harms, quality adjusted life year loss\n",
    "\n",
    "- C_FP undertreated pain when the patient would not have become addicted: pain-related disutility, extra visits, alternative therapies, short term function loss, patient dissatisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set domain costs\n",
    "C_FN = 100.0  # missed addiction case\n",
    "C_FP = 10.0   # undertreated pain\n",
    "\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "res = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "tbl = res[\"table\"].copy()\n",
    "tbl[\"expected_cost\"] = C_FP * tbl[\"FP\"] + C_FN * tbl[\"FN\"]\n",
    "\n",
    "# Plot expected cost vs threshold with the two candidate thresholds\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"expected_cost\"])\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Expected cost\")\n",
    "plt.title(\"Cost based threshold selection\")\n",
    "t_formula = res[\"threshold_formula\"]\n",
    "t_emp = res[\"threshold_empirical\"]\n",
    "plt.axvline(t_formula, linestyle=\"--\")\n",
    "plt.axvline(t_emp, linestyle=\":\")\n",
    "plt.show()\n",
    "\n",
    "# Compact comparison table\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "display(summary_df.round(3).set_index(\"rule\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81633a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52e712ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "015cf416",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc387e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Curves\n",
    "\n",
    "_ = RocCurveDisplay.from_predictions(y_test, proba_test)\n",
    "plt.title(\"ROC curve\")\n",
    "plt.show()\n",
    "\n",
    "_ = PrecisionRecallDisplay.from_predictions(y_test, proba_test)\n",
    "plt.title(\"Precision recall curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Quick programmatic fairness check by Low_inc\n",
    "\n",
    "sf = X_test[\"Low_inc\"]\n",
    "mf = MetricFrame(\n",
    "    metrics={\n",
    "        \"accuracy\": accuracy_score,\n",
    "        \"selection_rate\": selection_rate,\n",
    "        \"tpr\": true_positive_rate,\n",
    "        \"fpr\": false_positive_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=pred_test,\n",
    "    sensitive_features=sf\n",
    ")\n",
    "mf.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49714a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Responsible AI insights\n",
    "\n",
    "# Mark binary flags as categorical so the RAI dashboard can include data balance measures\n",
    "categorical_features = cat_like_binary_cols.copy()\n",
    "\n",
    "# RAIInsights expects training and test data with target included\n",
    "train_with_target = X_train.copy()\n",
    "train_with_target[target_col] = y_train\n",
    "\n",
    "test_with_target = X_test.copy()\n",
    "test_with_target[target_col] = y_test\n",
    "\n",
    "rai_insights = RAIInsights(\n",
    "    model=clf,\n",
    "    train=train_with_target,\n",
    "    test=test_with_target,\n",
    "    target_column=target_col,\n",
    "    task_type=\"classification\",\n",
    "    categorical_features=categorical_features\n",
    ")\n",
    "\n",
    "# Add components\n",
    "rai_insights.explainer.add()\n",
    "rai_insights.error_analysis.add()\n",
    "\n",
    "# Fairness assessment focused on Low_inc\n",
    "try:\n",
    "    rai_insights.fairness.add(sensitive_features=[\"Low_inc\"])\n",
    "except Exception as e:\n",
    "    print(\"Fairness add warning:\", e)\n",
    "\n",
    "# Optional components that may require extra dependencies\n",
    "try:\n",
    "    # Counterfactuals use DiCE under the hood\n",
    "    rai_insights.counterfactual.add(total_CFs=50, desired_class=\"opposite\")\n",
    "except Exception as e:\n",
    "    print(\"Counterfactuals not added:\", e)\n",
    "\n",
    "try:\n",
    "    # Causal analysis over a subset of features\n",
    "    treatment_features = [\"rx_ds\"]\n",
    "    heterogeneity_features = [\"Surgery\"]\n",
    "    rai_insights.causal.add(treatment_features=treatment_features,\n",
    "                            heterogeneity_features=heterogeneity_features)\n",
    "except Exception as e:\n",
    "    print(\"Causal analysis not added:\", e)\n",
    "\n",
    "rai_insights.compute()\n",
    "print(\"RAI insights computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Launch Responsible AI dashboard\n",
    "\n",
    "# The feature_flights argument can enable the data balance experience when categorical_features are present\n",
    "ResponsibleAIDashboard(rai_insights, feature_flights=\"dataBalanceExperience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save model and splits for reuse\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "out_dir = Path(\"./artifacts\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "joblib.dump(clf, out_dir / \"od_lr_pipeline.joblib\")\n",
    "\n",
    "meta = {\n",
    "    \"created\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"n_train\": int(X_train.shape[0]),\n",
    "    \"n_test\": int(X_test.shape[0]),\n",
    "    \"features\": list(X.columns),\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"target\": target_col,\n",
    "    \"metrics\": metrics\n",
    "}\n",
    "with open(out_dir / \"run_metadata.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", list(out_dir.iterdir()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4a28b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebed8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_rai_lgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
